{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5041f428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages...\n",
      "\n",
      "Fixing typing_extensions compatibility...\n",
      "\n",
      "Downloading spaCy Dutch model...\n",
      "\n",
      "‚úì Installation complete!\n",
      "\n",
      "‚ö† IMPORTANT: Restart kernel after installation:\n",
      "   Kernel ‚Üí Restart Kernel\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 0: Package Installation\n",
    "# ============================================\n",
    "# Install all required packages for the RAG system\n",
    "# Run this cell first, then restart kernel if needed\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "packages = [\n",
    "    \"flagembedding\",\n",
    "    \"sentence-transformers\",\n",
    "    \"spacy\",\n",
    "    \"pandas\",\n",
    "    \"openpyxl\",\n",
    "    \"numpy\",\n",
    "    \"tqdm\",\n",
    "    \"psycopg2-binary\",\n",
    "    \"sqlalchemy\",\n",
    "    \"elasticsearch\",\n",
    "    \"python-docx\"\n",
    "]\n",
    "\n",
    "print(\"Installing packages...\")\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", pkg], \n",
    "                         stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "# Fix typing_extensions compatibility\n",
    "print(\"\\nFixing typing_extensions compatibility...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"typing_extensions>=4.8.0\"],\n",
    "                     stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "# Download spaCy Dutch model\n",
    "print(\"\\nDownloading spaCy Dutch model...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"nl_core_news_md\"],\n",
    "                     stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "print(\"\\n‚úì Installation complete!\")\n",
    "print(\"\\n‚ö† IMPORTANT: Restart kernel after installation:\")\n",
    "print(\"   Kernel ‚Üí Restart Kernel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2cfb76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect to PostgreSQL...\n",
      "Connection string: localhost:5432/ecli\n",
      "‚úì Successfully connected to PostgreSQL\n",
      "\n",
      "Setting up database schema...\n",
      "‚úì Executed: CREATE EXTENSION IF NOT EXISTS vector;...\n",
      "‚úì Executed: CREATE TABLE IF NOT EXISTS documents (\n",
      "          d...\n",
      "‚úì Executed: -- Set this to your model's dimension; BGE-M3 dens...\n",
      "‚ö† Skipped (already exists): -- IVF index for approximate search\n",
      "        CREATE...\n",
      "‚ö† Skipped (already exists): CREATE INDEX chunks_doc_id_idx ON chunks (doc_id);...\n",
      "\n",
      "‚úì Database schema setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup PostgreSQL database schema\n",
    "# This cell creates the necessary tables and indexes for the RAG system\n",
    "# Set SKIP_DB_SETUP=True to skip database setup if PostgreSQL is not available\n",
    "\n",
    "import os\n",
    "from sqlalchemy import create_engine, text as sqltext\n",
    "from sqlalchemy.exc import OperationalError\n",
    "\n",
    "# Database connection string - update with your credentials\n",
    "PG_DSN = os.getenv(\"PG_DSN\", \"postgresql+psycopg2://postgres:postgres@localhost:5432/ecli\")\n",
    "SKIP_DB_SETUP = os.getenv(\"SKIP_DB_SETUP\", \"False\").lower() == \"true\"\n",
    "\n",
    "print(\"Attempting to connect to PostgreSQL...\")\n",
    "print(f\"Connection string: {PG_DSN.split('@')[1] if '@' in PG_DSN else 'hidden'}\")\n",
    "\n",
    "if SKIP_DB_SETUP:\n",
    "    print(\"‚ö† SKIP_DB_SETUP is True - skipping database setup\")\n",
    "    print(\"Set SKIP_DB_SETUP=False or unset it to enable database setup\")\n",
    "    engine = None\n",
    "else:\n",
    "    try:\n",
    "        engine = create_engine(PG_DSN)\n",
    "        # Test connection\n",
    "        with engine.connect() as test_conn:\n",
    "            test_conn.execute(sqltext(\"SELECT 1\"))\n",
    "        print(\"‚úì Successfully connected to PostgreSQL\")\n",
    "    except OperationalError as e:\n",
    "        print(\"‚úó Failed to connect to PostgreSQL\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"\\nTroubleshooting steps:\")\n",
    "        print(\"1. Check if PostgreSQL is running: sudo systemctl status postgresql\")\n",
    "        print(\"2. Start PostgreSQL if needed: sudo systemctl start postgresql\")\n",
    "        print(\"3. Verify the connection string in PG_DSN environment variable\")\n",
    "        print(\"4. Check if the database exists (create it with: createdb ecli)\")\n",
    "        print(\"5. Verify PostgreSQL is listening on port 5432\")\n",
    "        print(\"\\n‚ö† Continuing without database connection. Engine is None.\")\n",
    "        print(\"To skip database setup in future runs, set: import os; os.environ['SKIP_DB_SETUP'] = 'True'\")\n",
    "        engine = None\n",
    "\n",
    "# SQL commands to set up the database\n",
    "if engine is not None:\n",
    "    sql_commands = [\n",
    "        \"CREATE EXTENSION IF NOT EXISTS vector;\",\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS documents (\n",
    "          doc_id TEXT PRIMARY KEY,\n",
    "          source TEXT,\n",
    "          doc_type TEXT,\n",
    "          published_at TIMESTAMPTZ,\n",
    "          title TEXT,\n",
    "          language TEXT,\n",
    "          raw_metadata JSONB,\n",
    "          text TEXT\n",
    "        );\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        -- Set this to your model's dimension; BGE-M3 dense is 1024 by default in FlagEmbedding\n",
    "        CREATE TABLE IF NOT EXISTS chunks (\n",
    "          chunk_id TEXT PRIMARY KEY,\n",
    "          doc_id TEXT REFERENCES documents(doc_id),\n",
    "          idx INT,\n",
    "          language TEXT,\n",
    "          section_title TEXT,\n",
    "          text TEXT,\n",
    "          embedding vector(1024),\n",
    "          char_start INT,\n",
    "          char_end INT\n",
    "        );\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        -- IVF index for approximate search\n",
    "        CREATE INDEX chunks_embedding_idx ON chunks USING ivfflat (embedding vector_cosine_ops) WITH (lists = 200);\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        CREATE INDEX chunks_doc_id_idx ON chunks (doc_id);\n",
    "        \"\"\"\n",
    "    ]\n",
    "\n",
    "    # Execute SQL commands\n",
    "    print(\"\\nSetting up database schema...\")\n",
    "    # Execute each command in its own transaction to avoid transaction rollback issues\n",
    "    for sql in sql_commands:\n",
    "        try:\n",
    "            # Use separate transaction for each command\n",
    "            with engine.begin() as conn:\n",
    "                conn.execute(sqltext(sql))\n",
    "            print(f\"‚úì Executed: {sql.strip()[:50]}...\")\n",
    "        except Exception as e:\n",
    "            # Check if it's a \"already exists\" error\n",
    "            error_str = str(e).lower()\n",
    "            if (\"already exists\" in error_str or \"duplicate\" in error_str or \n",
    "                (\"relation\" in error_str and \"already exists\" in error_str)):\n",
    "                print(f\"‚ö† Skipped (already exists): {sql.strip()[:50]}...\")\n",
    "            else:\n",
    "                print(f\"‚ö† Warning: {sql.strip()[:50]}...\")\n",
    "                print(f\"   Error: {str(e)[:200]}\")\n",
    "                # Don't raise - continue with next command\n",
    "\n",
    "    print(\"\\n‚úì Database schema setup complete!\")\n",
    "else:\n",
    "    print(\"\\n‚ö† Database setup skipped - PostgreSQL not available. Engine is None.\")\n",
    "    print(\"To use database features, please start PostgreSQL and re-run this cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8962bbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data status...\n",
      "============================================================\n",
      "\n",
      "üìä PostgreSQL Data:\n",
      "   Document count: 3014\n",
      "   Chunk count: 27321\n",
      "\n",
      "   Sample documents:\n",
      "     - ECLI:NL:HR:2014:39: Document 1...\n",
      "     - ECLI:NL:HR:2020:848: Document 2...\n",
      "     - ECLI:NL:RVS:2020:64: Document 3...\n",
      "     - ECLI:NL:RVS:2023:3417: Document 4...\n",
      "     - ECLI:NL:RVS:2017:2310: Document 43...\n",
      "\n",
      "‚ùå Elasticsearch connection failed\n",
      "\n",
      "============================================================\n",
      "\n",
      "üí° Tips:\n",
      "   ‚úì Data is ready! You can run search tests now\n",
      "   Run: test_search('your query', top_n=5)\n",
      "   Or: find_relevant_ecli('advice letter text', top_n=10)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Check Data Status\n",
    "# ============================================\n",
    "# This cell checks if data exists in the system\n",
    "\n",
    "import os\n",
    "from sqlalchemy import create_engine, text as sqltext\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "PG_DSN = os.getenv(\"PG_DSN\", \"postgresql+psycopg2://postgres:postgres@localhost:5432/ecli\")\n",
    "ES_URL = os.getenv(\"ES_URL\", \"http://localhost:9200\")\n",
    "ES_INDEX = \"ecli_chunks\"\n",
    "\n",
    "print(\"Checking data status...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize variables (important: set defaults before try block)\n",
    "doc_count = 0\n",
    "chunk_count = 0\n",
    "es_count = 0\n",
    "\n",
    "# Check PostgreSQL\n",
    "try:\n",
    "    engine = create_engine(PG_DSN)\n",
    "    with engine.connect() as conn:\n",
    "        # First check if tables exist\n",
    "        tables_check = conn.execute(sqltext(\"\"\"\n",
    "            SELECT table_name \n",
    "            FROM information_schema.tables \n",
    "            WHERE table_schema = 'public' \n",
    "            AND table_name IN ('documents', 'chunks')\n",
    "        \"\"\")).fetchall()\n",
    "        existing_tables = [t[0] for t in tables_check]\n",
    "        \n",
    "        if 'documents' not in existing_tables or 'chunks' not in existing_tables:\n",
    "            print(f\"\\n‚ö† PostgreSQL tables not found:\")\n",
    "            print(f\"   - documents table: {'‚úì exists' if 'documents' in existing_tables else '‚úó missing'}\")\n",
    "            print(f\"   - chunks table: {'‚úì exists' if 'chunks' in existing_tables else '‚úó missing'}\")\n",
    "            print(f\"\\n   Please run Cell 2 (Database Setup) first to create the tables.\")\n",
    "            doc_count = 0\n",
    "            chunk_count = 0\n",
    "        else:\n",
    "            # Check document count\n",
    "            doc_count = conn.execute(sqltext(\"SELECT COUNT(*) FROM documents\")).scalar()\n",
    "            chunk_count = conn.execute(sqltext(\"SELECT COUNT(*) FROM chunks\")).scalar()\n",
    "            \n",
    "            print(f\"\\nüìä PostgreSQL Data:\")\n",
    "            print(f\"   Document count: {doc_count}\")\n",
    "            print(f\"   Chunk count: {chunk_count}\")\n",
    "            \n",
    "            if doc_count > 0:\n",
    "                # Show some sample documents\n",
    "                samples = conn.execute(sqltext(\"SELECT doc_id, title FROM documents LIMIT 5\")).mappings().all()\n",
    "                print(f\"\\n   Sample documents:\")\n",
    "                for s in samples:\n",
    "                    title = s['title'][:50] if s['title'] else 'No title'\n",
    "                    print(f\"     - {s['doc_id']}: {title}...\")\n",
    "            else:\n",
    "                print(\"   ‚ö† No documents found - run import_all_data() to import data\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå PostgreSQL connection failed: {e}\")\n",
    "    print(\"   Please ensure PostgreSQL is running\")\n",
    "    print(\"   If tables don't exist, run Cell 2 (Database Setup) first\")\n",
    "    doc_count = 0  # Ensure variable is set even on error\n",
    "    chunk_count = 0\n",
    "\n",
    "# Check Elasticsearch\n",
    "try:\n",
    "    es = Elasticsearch(ES_URL)\n",
    "    if es.ping():\n",
    "        es_count = es.count(index=ES_INDEX)['count']\n",
    "        print(f\"\\nüìä Elasticsearch Data:\")\n",
    "        print(f\"   Indexed chunk count: {es_count}\")\n",
    "        if es_count == 0:\n",
    "            print(\"   ‚ö† Elasticsearch index is empty\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Elasticsearch connection failed\")\n",
    "        es_count = 0\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Elasticsearch connection failed: {e}\")\n",
    "    print(\"   Please ensure Elasticsearch is running\")\n",
    "    es_count = 0  # Ensure variable is set even on error\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüí° Tips:\")\n",
    "if doc_count == 0:\n",
    "    print(\"   1. You need to import data first to test search functionality\")\n",
    "    print(\"   2. If you have Excel files, run: import_all_data()\")\n",
    "    print(\"   3. If you have text files, run: main(data_source='data')\")\n",
    "else:\n",
    "    print(\"   ‚úì Data is ready! You can run search tests now\")\n",
    "    print(\"   Run: test_search('your query', top_n=5)\")\n",
    "    print(\"   Or: find_relevant_ecli('advice letter text', top_n=10)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5285998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, uuid, math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"nl_core_news_md\")\n",
    "\n",
    "def simple_lang(doc_text:str)->str:\n",
    "    # If you know it's Dutch, return \"nl\"; you can integrate a detector later.\n",
    "    return \"nl\"\n",
    "\n",
    "def chunk_text(text, target_tokens=350, overlap=80):\n",
    "    \"\"\"\n",
    "    Sentence-aware chunking using spaCy. Keeps heading+following paragraph if detected.\n",
    "    \n",
    "    ‚ö†Ô∏è  IMPORTANT: If you change these parameters, you MUST re-import data!\n",
    "    Changing chunking parameters will NOT automatically update existing data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    target_tokens : int\n",
    "        Target number of tokens per chunk (default: 350)\n",
    "    overlap : int\n",
    "        Number of tokens to overlap between chunks (default: 80)\n",
    "    \n",
    "    To apply new chunking:\n",
    "    1. Modify target_tokens and/or overlap above\n",
    "    2. Run clear_all_chunks() to delete old chunks\n",
    "    3. Run import_all_data() to re-import with new chunking\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    sents = [s.text.strip() for s in doc.sents if s.text.strip()]\n",
    "    chunks, cur, cur_len = [], [], 0\n",
    "    for s in sents:\n",
    "        # approximate token length by whitespace split (good enough to start)\n",
    "        s_len = len(s.split())\n",
    "        if cur_len + s_len > target_tokens and cur:\n",
    "            chunks.append(\" \".join(cur))\n",
    "            # overlap from end of cur\n",
    "            if overlap > 0:\n",
    "                tail = \" \".join(\" \".join(cur).split()[-overlap:])\n",
    "                cur, cur_len = [tail], len(tail.split())\n",
    "            else:\n",
    "                cur, cur_len = [], 0\n",
    "        cur.append(s); cur_len += s_len\n",
    "    if cur:\n",
    "        chunks.append(\" \".join(cur))\n",
    "    return chunks\n",
    "\n",
    "def load_docs_from_folder(folder=\"data\"):\n",
    "    \"\"\"Expect raw .txt (or pre-extracted from PDFs). You can extend to PDFs later.\"\"\"\n",
    "    paths = list(Path(folder).glob(\"**/*.txt\"))\n",
    "    for p in paths:\n",
    "        text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        meta_path = p.with_suffix(\".meta.json\")\n",
    "        meta = json.loads(meta_path.read_text()) if meta_path.exists() else {}\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        yield {\n",
    "            \"doc_id\": doc_id,\n",
    "            \"title\": meta.get(\"title\") or p.stem,\n",
    "            \"source\": meta.get(\"source\") or str(p),\n",
    "            \"doc_type\": meta.get(\"doc_type\") or \"advice_or_ecli\",\n",
    "            \"published_at\": meta.get(\"published_at\"),\n",
    "            \"language\": simple_lang(text),\n",
    "            \"raw_metadata\": meta,\n",
    "            \"text\": text\n",
    "        }\n",
    "\n",
    "def load_docs_from_excel(excel_path, text_column=None, id_column=None, title_column=None, metadata_columns=None):\n",
    "    \"\"\"\n",
    "    Load documents from Excel file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    excel_path : str\n",
    "        Path to Excel file (.xlsx or .xls)\n",
    "    text_column : str, optional\n",
    "        Column name containing the document text. If None, tries to find common names.\n",
    "    id_column : str, optional\n",
    "        Column name for document ID (e.g., ECLI number). If None, generates UUID.\n",
    "    title_column : str, optional\n",
    "        Column name for title. If None, uses first non-empty text column.\n",
    "    metadata_columns : list, optional\n",
    "        List of column names to include in raw_metadata\n",
    "    \n",
    "    Yields:\n",
    "    -------\n",
    "    dict : Document dictionary with doc_id, title, source, doc_type, text, etc.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Read Excel file\n",
    "    df = pd.read_excel(excel_path)\n",
    "    \n",
    "    # Auto-detect columns if not specified\n",
    "    if text_column is None:\n",
    "        # Try common column names for text\n",
    "        text_candidates = [col for col in df.columns if any(keyword in col.lower() \n",
    "                          for keyword in ['text', 'content', 'body', 'document', 'advice', 'letter'])]\n",
    "        text_column = text_candidates[0] if text_candidates else df.columns[0]\n",
    "    \n",
    "    if id_column is None:\n",
    "        # Try to find ECLI or ID column\n",
    "        id_candidates = [col for col in df.columns if any(keyword in col.lower() \n",
    "                        for keyword in ['ecli', 'id', 'nummer', 'number', 'case_id'])]\n",
    "        id_column = id_candidates[0] if id_candidates else None\n",
    "    \n",
    "    if title_column is None:\n",
    "        title_candidates = [col for col in df.columns if any(keyword in col.lower() \n",
    "                           for keyword in ['title', 'titel', 'subject', 'onderwerp'])]\n",
    "        title_column = title_candidates[0] if title_candidates else None\n",
    "    \n",
    "    # Extract ECLI numbers from text if present\n",
    "    def extract_ecli(text):\n",
    "        \"\"\"Extract ECLI identifier from text (format: ECLI:XX:YYYY:ZZZZZZZZ)\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return None\n",
    "        text_str = str(text)\n",
    "        # Match ECLI pattern: ECLI:XX:YYYY:ZZZZZZZZ\n",
    "        ecli_pattern = r'ECLI:[A-Z]{2}:[A-Z0-9]{4}:[A-Z0-9]+'\n",
    "        matches = re.findall(ecli_pattern, text_str, re.IGNORECASE)\n",
    "        return matches[0] if matches else None\n",
    "    \n",
    "    # Process each row\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row[text_column] if text_column in row else ''\n",
    "        if pd.isna(text) or str(text).strip() == '':\n",
    "            continue  # Skip empty rows\n",
    "        \n",
    "        # Get document ID (ECLI number or generate UUID)\n",
    "        if id_column and id_column in row and not pd.isna(row[id_column]):\n",
    "            doc_id = str(row[id_column]).strip()\n",
    "        else:\n",
    "            # Try to extract ECLI from text\n",
    "            ecli = extract_ecli(text)\n",
    "            doc_id = ecli if ecli else str(uuid.uuid4())\n",
    "        \n",
    "        # Get title\n",
    "        if title_column and title_column in row and not pd.isna(row[title_column]):\n",
    "            title = str(row[title_column])\n",
    "        else:\n",
    "            title = f\"Document {idx + 1}\"\n",
    "        \n",
    "        # Build metadata from specified columns or all non-text columns\n",
    "        if metadata_columns:\n",
    "            meta = {col: row[col] for col in metadata_columns if col in row}\n",
    "        else:\n",
    "            # Include all columns except text column in metadata\n",
    "            meta = {col: row[col] for col in df.columns \n",
    "                   if col != text_column and not pd.isna(row[col])}\n",
    "        \n",
    "        # Extract ECLI if present in text\n",
    "        ecli_in_text = extract_ecli(text)\n",
    "        if ecli_in_text:\n",
    "            meta['ecli'] = ecli_in_text\n",
    "        \n",
    "        yield {\n",
    "            \"doc_id\": doc_id,\n",
    "            \"title\": title,\n",
    "            \"source\": str(excel_path),\n",
    "            \"doc_type\": \"ecli\" if \"ecli\" in str(doc_id).lower() or ecli_in_text else \"advice_or_ecli\",\n",
    "            \"published_at\": meta.get(\"published_at\") or meta.get(\"date\") or meta.get(\"datum\"),\n",
    "            \"language\": simple_lang(str(text)),\n",
    "            \"raw_metadata\": meta,\n",
    "            \"text\": str(text)\n",
    "        }\n",
    "\n",
    "def build_chunks(doc):\n",
    "    chunks = chunk_text(doc[\"text\"], target_tokens=350, overlap=80)\n",
    "    out = []\n",
    "    start = 0\n",
    "    for i, ch in enumerate(chunks):\n",
    "        end = start + len(ch)\n",
    "        out.append({\n",
    "            \"chunk_id\": str(uuid.uuid4()),\n",
    "            \"doc_id\": doc[\"doc_id\"],\n",
    "            \"idx\": i,\n",
    "            \"language\": doc[\"language\"],\n",
    "            \"section_title\": doc[\"raw_metadata\"].get(\"section_title\") if isinstance(doc[\"raw_metadata\"], dict) else None,\n",
    "            \"text\": ch,\n",
    "            \"char_start\": start,\n",
    "            \"char_end\": end\n",
    "        })\n",
    "        start = end\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3051b49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/990/xue_gnn/env_gnn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì FlagEmbedding imported successfully\n",
      "‚úì elasticsearch imported successfully\n",
      "\n",
      "Loading BGE-M3 model (this may take a moment on first run)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 56299.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded successfully\n",
      "\n",
      "üí° Model info:\n",
      "   - Model: BAAI/bge-m3\n",
      "   - Embedding dimension: 1024\n",
      "   - If you change this, you must re-generate embeddings!\n",
      "\n",
      "Loading Reranker model (this may take a moment on first run)...\n",
      "‚úì Reranker loaded successfully\n",
      "   - Model: BAAI/bge-reranker-base\n",
      "   - Will rerank top 50 candidates to get top 5 results\n",
      "‚Ñπ Elasticsearch disabled (using PostgreSQL only)\n",
      "\n",
      "‚úì Configuration complete! Ready for data import and search.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 4: Model Loading & Configuration\n",
    "# ============================================\n",
    "# Load BGE-M3 embedding model and initialize database connections\n",
    "# This cell must be run after Cell 0 (package installation) and Cell 1 (database setup)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from sqlalchemy import create_engine, text as sqltext\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "# Import FlagEmbedding (IMPORTANT: capitalized, not lowercase)\n",
    "try:\n",
    "    from FlagEmbedding import BGEM3FlagModel\n",
    "    print(\"‚úì FlagEmbedding imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚úó FlagEmbedding not found. Installing...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"flagembedding\"],\n",
    "                         stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    print(\"‚úì Package installed. Please RESTART KERNEL and re-run this cell.\")\n",
    "    raise ImportError(\"Please restart kernel after installation\")\n",
    "\n",
    "# Check elasticsearch\n",
    "try:\n",
    "    from elasticsearch import Elasticsearch, helpers\n",
    "    print(\"‚úì elasticsearch imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚úó elasticsearch not found. Installing...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"elasticsearch\"],\n",
    "                         stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    print(\"‚úì Package installed. Please RESTART KERNEL and re-run this cell.\")\n",
    "    raise ImportError(\"Please restart kernel after installation\")\n",
    "\n",
    "# === Configuration ===\n",
    "PG_DSN = os.getenv(\"PG_DSN\", \"postgresql+psycopg2://postgres:postgres@localhost:5432/ecli\")\n",
    "USE_ELASTIC = False  # Set to False to disable Elasticsearch (use PostgreSQL only)\n",
    "ES_URL = os.getenv(\"ES_URL\", \"http://localhost:9200\")\n",
    "ES_INDEX = \"ecli_chunks\"\n",
    "\n",
    "# Load BGE-M3 model\n",
    "# ‚ö†Ô∏è  IMPORTANT: If you change the model or model parameters, you MUST re-generate embeddings!\n",
    "# Changing the model will NOT automatically update existing embeddings in the database.\n",
    "#\n",
    "# To apply a new model:\n",
    "# 1. Modify the model below (e.g., change \"BAAI/bge-m3\" to a different model)\n",
    "# 2. If embedding dimension changes, update the database schema (Cell 2): vector(1024) ‚Üí vector(NEW_DIM)\n",
    "# 3. Run clear_all_chunks(confirm=True) to delete old chunks and embeddings\n",
    "# 4. Run import_all_data() to re-generate embeddings with the new model\n",
    "\n",
    "print(\"\\nLoading BGE-M3 model (this may take a moment on first run)...\")\n",
    "model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True)  # switches to GPU if available\n",
    "print(\"‚úì Model loaded successfully\")\n",
    "print(\"\\nüí° Model info:\")\n",
    "print(f\"   - Model: BAAI/bge-m3\")\n",
    "print(f\"   - Embedding dimension: 1024\")\n",
    "print(f\"   - If you change this, you must re-generate embeddings!\")\n",
    "\n",
    "# Load Reranker model (optional, for improving precision)\n",
    "# Reranker is used to re-rank initial search results for better accuracy\n",
    "USE_RERANKER = True  # Set to False to disable reranker\n",
    "reranker = None\n",
    "\n",
    "if USE_RERANKER:\n",
    "    try:\n",
    "        from FlagEmbedding import FlagReranker\n",
    "        print(\"\\nLoading Reranker model (this may take a moment on first run)...\")\n",
    "        reranker = FlagReranker('BAAI/bge-reranker-base', use_fp16=True)\n",
    "        print(\"‚úì Reranker loaded successfully\")\n",
    "        print(\"   - Model: BAAI/bge-reranker-base\")\n",
    "        print(\"   - Will rerank top 50 candidates to get top 5 results\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö† Reranker not available (FlagReranker not found)\")\n",
    "        print(\"   Continuing without reranker...\")\n",
    "        USE_RERANKER = False\n",
    "        reranker = None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Failed to load reranker: {e}\")\n",
    "        print(\"   Continuing without reranker...\")\n",
    "        USE_RERANKER = False\n",
    "        reranker = None\n",
    "else:\n",
    "    print(\"\\n‚Ñπ Reranker disabled (USE_RERANKER=False)\")\n",
    "\n",
    "# Initialize database engine\n",
    "engine = create_engine(PG_DSN)\n",
    "\n",
    "# Initialize Elasticsearch with error handling\n",
    "es = None\n",
    "if USE_ELASTIC:\n",
    "    try:\n",
    "        es = Elasticsearch(ES_URL, request_timeout=5)\n",
    "        if es.ping(request_timeout=2):\n",
    "            print(\"‚úì Elasticsearch connected\")\n",
    "        else:\n",
    "            print(\"‚ö† Elasticsearch not available (will use PostgreSQL only)\")\n",
    "            es = None\n",
    "    except Exception:\n",
    "        print(\"‚ö† Elasticsearch connection failed (will use PostgreSQL only)\")\n",
    "        es = None\n",
    "else:\n",
    "    print(\"‚Ñπ Elasticsearch disabled (using PostgreSQL only)\")\n",
    "\n",
    "print(\"\\n‚úì Configuration complete! Ready for data import and search.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bc7bc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Import function ready!\n",
      "\n",
      "======================================================================\n",
      "‚ö†Ô∏è  IMPORTANT: Run the cell below to actually import data!\n",
      "======================================================================\n",
      "\n",
      "To import both files, run:\n",
      "  import_all_data()\n",
      "\n",
      "This will:\n",
      "  1. Import ECLI documents (knowledge base)\n",
      "  2. Import Advice Letter documents (for testing)\n",
      "  3. Create embeddings and store in PostgreSQL\n",
      "  4. This may take 20-30 minutes depending on data size\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Import both ECLI and Advice Letter data\n",
    "# ============================================\n",
    "# This cell imports both Excel files:\n",
    "# 1. ECLI data (knowledge base for citation)\n",
    "# 2. Advice Letter data (for testing queries)\n",
    "\n",
    "def import_all_data():\n",
    "    \"\"\"\n",
    "    Import both ECLI data and Advice Letter data into the RAG system.\n",
    "    \n",
    "    ECLI data will be used as the knowledge base for citation.\n",
    "    Advice letters will be used for testing queries.\n",
    "    \"\"\"\n",
    "    # Check if engine exists and can connect\n",
    "    if engine is None:\n",
    "        print(\"‚úó Cannot import: database engine is None\")\n",
    "        print(\"   Please ensure PostgreSQL is running and Cell 2 completed successfully\")\n",
    "        print(\"\\n   To start PostgreSQL:\")\n",
    "        print(\"   sudo systemctl start postgresql\")\n",
    "        print(\"   # Or if using Docker: docker run -d -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\")\n",
    "        return\n",
    "    \n",
    "    # Test connection before proceeding\n",
    "    try:\n",
    "        with engine.connect() as test_conn:\n",
    "            test_conn.execute(sqltext(\"SELECT 1\"))\n",
    "        print(\"‚úì Database connection verified\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Cannot connect to PostgreSQL: {e}\")\n",
    "        print(\"\\n   Troubleshooting:\")\n",
    "        print(\"   1. Check if PostgreSQL is installed:\")\n",
    "        print(\"      sudo apt-get install postgresql postgresql-contrib\")\n",
    "        print(\"   2. Start PostgreSQL service:\")\n",
    "        print(\"      sudo systemctl start postgresql\")\n",
    "        print(\"      sudo systemctl enable postgresql\")\n",
    "        print(\"   3. Create database:\")\n",
    "        print(\"      sudo -u postgres createdb ecli\")\n",
    "        print(\"   4. Or use Docker:\")\n",
    "        print(\"      docker run -d --name postgres-ecli -p 5432:5432 -e POSTGRES_PASSWORD=postgres postgres:15\")\n",
    "        print(\"      docker exec -it postgres-ecli psql -U postgres -c 'CREATE DATABASE ecli;'\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"Importing ECLI data (Knowledge Base)...\")\n",
    "    \n",
    "    # Get absolute path to Excel files in current directory\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Use current working directory (where notebook is located)\n",
    "    base_path = Path.cwd()\n",
    "    print(f\"Current working directory: {base_path}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Import ECLI data\n",
    "    # Get absolute path to Excel files in current directory\n",
    "    import os\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd()\n",
    "    # Use Path for cross-platform compatibility\n",
    "    from pathlib import Path\n",
    "    base_path = Path.cwd()  # Current working directory (where notebook is)\n",
    "    \n",
    "    ecli_file = base_path / \"DATA ecli_nummers juni 2025 v1 (version 1).xlsx\"\n",
    "    if not ecli_file.exists():\n",
    "        print(f\"‚úó ECLI file not found: {ecli_file}\")\n",
    "        print(f\"   Current directory: {base_path}\")\n",
    "        return\n",
    "    ecli_file = str(ecli_file)  # Convert to string for pandas\n",
    "    \n",
    "    # Auto-detect columns for ECLI file\n",
    "    import pandas as pd\n",
    "    df_ecli = pd.read_excel(ecli_file, nrows=1)\n",
    "    \n",
    "    # Find text column (likely \"ecli_tekst\")\n",
    "    text_col = None\n",
    "    for col in [\"ecli_tekst\", \"text\", \"content\", \"document_text\"]:\n",
    "        if col in df_ecli.columns:\n",
    "            text_col = col\n",
    "            break\n",
    "    if not text_col:\n",
    "        text_col = df_ecli.columns[5] if len(df_ecli.columns) > 5 else df_ecli.columns[-1]\n",
    "    \n",
    "    # Find ID column (ecli_nummer)\n",
    "    id_col = None\n",
    "    for col in [\"ecli_nummer\", \"ecli\", \"id\", \"doc_id\"]:\n",
    "        if col in df_ecli.columns:\n",
    "            id_col = col\n",
    "            break\n",
    "    if not id_col:\n",
    "        id_col = df_ecli.columns[0]\n",
    "    \n",
    "    print(f\"ECLI file - Text column: {text_col}, ID column: {id_col}\")\n",
    "    \n",
    "    # Import ECLI data with doc_type=\"ecli\"\n",
    "    doc_generator = load_docs_from_excel(\n",
    "        excel_path=ecli_file,\n",
    "        text_column=text_col,\n",
    "        id_column=id_col,\n",
    "        title_column=None\n",
    "    )\n",
    "    \n",
    "    # Modify documents to set doc_type=\"ecli\"\n",
    "    ecli_count = 0\n",
    "    with engine.begin() as conn:\n",
    "        for doc in tqdm(doc_generator, desc=\"Processing ECLI documents\"):\n",
    "            doc[\"doc_type\"] = \"ecli\"  # Mark as ECLI document\n",
    "            upsert_document(conn, doc)\n",
    "            chunks = build_chunks(doc)\n",
    "            if not chunks:\n",
    "                continue\n",
    "            insert_chunks(conn, chunks)\n",
    "            # Try to index to Elasticsearch, but don't fail if it's not available\n",
    "            try:\n",
    "                bulk_index_es(chunks)\n",
    "            except Exception:\n",
    "                pass  # Continue even if Elasticsearch fails\n",
    "            ecli_count += 1\n",
    "    \n",
    "    print(f\"\\n‚úì Imported {ecli_count} ECLI documents\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Importing Advice Letter data...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Import Advice Letter data\n",
    "    advice_file = base_path / \"Dataset Advice letters on objections towing of bicycles.xlsx\"\n",
    "    if not advice_file.exists():\n",
    "        print(f\"‚úó Advice file not found: {advice_file}\")\n",
    "        print(f\"   Current directory: {base_path}\")\n",
    "        return\n",
    "    advice_file = str(advice_file)  # Convert to string for pandas\n",
    "    \n",
    "    df_advice = pd.read_excel(advice_file, nrows=1)\n",
    "    \n",
    "    # Find text column (likely \"geanonimiseerd_doc_inhoud\")\n",
    "    text_col_advice = None\n",
    "    for col in [\"geanonimiseerd_doc_inhoud\", \"text\", \"content\", \"document_text\", \"doc_inhoud\"]:\n",
    "        if col in df_advice.columns:\n",
    "            text_col_advice = col\n",
    "            break\n",
    "    if not text_col_advice:\n",
    "        text_col_advice = df_advice.columns[4] if len(df_advice.columns) > 4 else df_advice.columns[-1]\n",
    "    \n",
    "    # Find ID column\n",
    "    id_col_advice = None\n",
    "    for col in [\"Octopus zaaknummer\", \"zaaknummer\", \"id\", \"doc_id\"]:\n",
    "        if col in df_advice.columns:\n",
    "            id_col_advice = col\n",
    "            break\n",
    "    if not id_col_advice:\n",
    "        id_col_advice = df_advice.columns[0]\n",
    "    \n",
    "    print(f\"Advice file - Text column: {text_col_advice}, ID column: {id_col_advice}\")\n",
    "    \n",
    "    # Import Advice Letter data with doc_type=\"advice\"\n",
    "    doc_generator_advice = load_docs_from_excel(\n",
    "        excel_path=advice_file,\n",
    "        text_column=text_col_advice,\n",
    "        id_column=id_col_advice,\n",
    "        title_column=\"Onderwerp\" if \"Onderwerp\" in df_advice.columns else None\n",
    "    )\n",
    "    \n",
    "    advice_count = 0\n",
    "    with engine.begin() as conn:\n",
    "        for doc in tqdm(doc_generator_advice, desc=\"Processing Advice documents\"):\n",
    "            doc[\"doc_type\"] = \"advice\"  # Mark as Advice document\n",
    "            upsert_document(conn, doc)\n",
    "            chunks = build_chunks(doc)\n",
    "            if not chunks:\n",
    "                continue\n",
    "            insert_chunks(conn, chunks)\n",
    "            # Try to index to Elasticsearch, but don't fail if it's not available\n",
    "            try:\n",
    "                bulk_index_es(chunks)\n",
    "            except Exception:\n",
    "                pass  # Continue even if Elasticsearch fails\n",
    "            advice_count += 1\n",
    "    \n",
    "    print(f\"\\n‚úì Imported {advice_count} Advice Letter documents\")\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Import complete! Total: {ecli_count} ECLI + {advice_count} Advice = {ecli_count + advice_count} documents\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "def clear_all_chunks(confirm=False):\n",
    "    \"\"\"\n",
    "    Clear all chunks from the database while keeping documents.\n",
    "    \n",
    "    ‚ö†Ô∏è  WARNING: This will delete all chunks and their embeddings!\n",
    "    Use this when you want to:\n",
    "    - Re-import data with different chunking parameters\n",
    "    - Re-generate embeddings with a different model\n",
    "    - Re-generate embeddings with different model parameters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    confirm : bool\n",
    "        Must be True to actually delete (safety check)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    int : Number of chunks deleted\n",
    "    \"\"\"\n",
    "    if not confirm:\n",
    "        print(\"‚ö†Ô∏è  Safety check: This function will delete ALL chunks!\")\n",
    "        print(\"   To confirm, run: clear_all_chunks(confirm=True)\")\n",
    "        print(\"\\n   This is useful when:\")\n",
    "        print(\"   - You changed chunking parameters (target_tokens, overlap)\")\n",
    "        print(\"   - You want to re-import data with new chunking strategy\")\n",
    "        return None\n",
    "    \n",
    "    if engine is None:\n",
    "        print(\"‚úó Cannot clear chunks: database engine is None\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # Count chunks before deletion\n",
    "            chunk_count = conn.execute(sqltext(\"SELECT COUNT(*) FROM chunks\")).scalar()\n",
    "            \n",
    "            # Delete all chunks\n",
    "            conn.execute(sqltext(\"DELETE FROM chunks\"))\n",
    "            \n",
    "            print(f\"‚úì Deleted {chunk_count} chunks from database\")\n",
    "            print(\"\\nüí° Next steps:\")\n",
    "            print(\"   1. Modify chunking parameters in Cell 3 if needed (target_tokens, overlap)\")\n",
    "            print(\"   2. Modify model in Cell 4 if needed (model name, parameters)\")\n",
    "            print(\"   3. If embedding dimension changed, update database schema in Cell 2\")\n",
    "            print(\"   4. Run import_all_data() to re-import with new settings\")\n",
    "            return chunk_count\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error clearing chunks: {e}\")\n",
    "        return None\n",
    "\n",
    "def clear_all_data(confirm=False):\n",
    "    \"\"\"\n",
    "    Clear ALL data from the database (documents AND chunks).\n",
    "    \n",
    "    ‚ö†Ô∏è  WARNING: This will delete EVERYTHING!\n",
    "    Use this only if you want to start completely fresh.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    confirm : bool\n",
    "        Must be True to actually delete (safety check)\n",
    "    \"\"\"\n",
    "    if not confirm:\n",
    "        print(\"‚ö†Ô∏è  DANGER: This will delete ALL documents and chunks!\")\n",
    "        print(\"   To confirm, run: clear_all_data(confirm=True)\")\n",
    "        return None\n",
    "    \n",
    "    if engine is None:\n",
    "        print(\"‚úó Cannot clear data: database engine is None\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # Count before deletion\n",
    "            doc_count = conn.execute(sqltext(\"SELECT COUNT(*) FROM documents\")).scalar()\n",
    "            chunk_count = conn.execute(sqltext(\"SELECT COUNT(*) FROM chunks\")).scalar()\n",
    "            \n",
    "            # Delete chunks first (foreign key constraint)\n",
    "            conn.execute(sqltext(\"DELETE FROM chunks\"))\n",
    "            # Then delete documents\n",
    "            conn.execute(sqltext(\"DELETE FROM documents\"))\n",
    "            \n",
    "            print(f\"‚úì Deleted {doc_count} documents and {chunk_count} chunks\")\n",
    "            print(\"\\nüí° Next steps:\")\n",
    "            print(\"   1. Modify chunking parameters in Cell 3 if needed\")\n",
    "            print(\"   2. Run import_all_data() to import fresh data\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error clearing data: {e}\")\n",
    "\n",
    "print(\"‚úì Import function ready!\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚ö†Ô∏è  IMPORTANT: Run the cell below to actually import data!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTo import both files, run:\")\n",
    "print(\"  import_all_data()\")\n",
    "print(\"\\nThis will:\")\n",
    "print(\"  1. Import ECLI documents (knowledge base)\")\n",
    "print(\"  2. Import Advice Letter documents (for testing)\")\n",
    "print(\"  3. Create embeddings and store in PostgreSQL\")\n",
    "print(\"  4. This may take 20-30 minutes depending on data size\")\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdb3c9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Popular ECLI functions ready!\n",
      "\n",
      "To get popular ECLI list:\n",
      "  popular_ecli = get_popular_ecli(min_citations=10)\n",
      "  for ecli, count in popular_ecli[:5]:\n",
      "      print(f'{ecli}: cited {count} times')\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# High-Citation ECLI Support Functions\n",
    "# ============================================\n",
    "# These functions add support for including popular ECLI in search results\n",
    "\n",
    "def get_popular_ecli(min_citations=10, top_k=None):\n",
    "    \"\"\"\n",
    "    Get popular ECLI numbers based on ground truth citation frequency.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    min_citations : int\n",
    "        Minimum number of citations to be considered \"popular\" (default: 10)\n",
    "    top_k : int, optional\n",
    "        If specified, return only top K most cited ECLI\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : List of tuples (ecli_number, citation_count) sorted by frequency\n",
    "    \"\"\"\n",
    "    ground_truth = load_ground_truth_ecli()\n",
    "    if not ground_truth:\n",
    "        return []\n",
    "    \n",
    "    # Count ECLI frequency in ground truth\n",
    "    ecli_frequency = {}\n",
    "    for zaaknummer, ecli_list in ground_truth.items():\n",
    "        for ecli in ecli_list:\n",
    "            ecli_frequency[ecli] = ecli_frequency.get(ecli, 0) + 1\n",
    "    \n",
    "    # Filter by minimum citations\n",
    "    popular_ecli = [(ecli, count) for ecli, count in ecli_frequency.items() \n",
    "                    if count >= min_citations]\n",
    "    \n",
    "    # Sort by frequency (descending)\n",
    "    popular_ecli.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top K if specified\n",
    "    if top_k:\n",
    "        popular_ecli = popular_ecli[:top_k]\n",
    "    \n",
    "    return popular_ecli\n",
    "\n",
    "print(\"‚úì Popular ECLI functions ready!\")\n",
    "print(\"\\nTo get popular ECLI list:\")\n",
    "print(\"  popular_ecli = get_popular_ecli(min_citations=10)\")\n",
    "print(\"  for ecli, count in popular_ecli[:5]:\")\n",
    "print(\"      print(f'{ecli}: cited {count} times')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d3a834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ‚ö†Ô∏è  EXECUTE THIS CELL TO IMPORT DATA\n",
    "# ============================================\n",
    "# This cell actually runs the import function\n",
    "# Make sure you've run the cell above (Cell 5) that defines import_all_data()\n",
    "# \n",
    "# This will import:\n",
    "# - ECLI documents (knowledge base for citation)\n",
    "# - Advice Letter documents (for testing)\n",
    "# \n",
    "# ‚è±Ô∏è  This may take 20-30 minutes depending on data size\n",
    "\n",
    "#import_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab51be57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Parameter testing function ready!\n",
      "\n",
      "To test different combinations:\n",
      "  test_parameter_combinations()\n",
      "\n",
      "‚ö†Ô∏è  Note: This will take a while as it tests multiple combinations\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Test Different Parameter Combinations\n",
    "# ============================================\n",
    "# Find optimal balance between Precision and Recall\n",
    "\n",
    "def test_parameter_combinations():\n",
    "    \"\"\"\n",
    "    Test different combinations of top_k and min_score to find optimal balance.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"Testing Different Parameter Combinations\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nThis will test multiple combinations and show the trade-offs.\")\n",
    "    print(\"Note: This may take a while...\\n\")\n",
    "    \n",
    "    # Test combinations\n",
    "    combinations = [\n",
    "        (5, 0.3, \"Fewer results, lower threshold\"),\n",
    "        (5, 0.35, \"Fewer results, medium threshold\"),\n",
    "        (5, 0.4, \"Fewer results, higher threshold\"),\n",
    "        (7, 0.3, \"Medium results, lower threshold\"),\n",
    "        (7, 0.35, \"Medium results, medium threshold\"),\n",
    "        (10, 0.3, \"More results, lower threshold\"),\n",
    "        (10, 0.35, \"More results, medium threshold\"),\n",
    "        (10, 0.4, \"More results, higher threshold\"),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for top_k, min_score, description in combinations:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Testing: top_k={top_k}, min_score={min_score}\")\n",
    "        print(f\"Description: {description}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        try:\n",
    "            metrics = evaluate_all_advice_letters(top_k=top_k, min_score=min_score)\n",
    "            if metrics:\n",
    "                results.append({\n",
    "                    'top_k': top_k,\n",
    "                    'min_score': min_score,\n",
    "                    'description': description,\n",
    "                    'precision': metrics.get('precision', 0),\n",
    "                    'recall': metrics.get('recall', 0),\n",
    "                    'accuracy': metrics.get('accuracy', 0),\n",
    "                    'f1': metrics.get('f1', 0),\n",
    "                    'mrr': metrics.get('mrr', 0)\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Error testing {top_k}/{min_score}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Summary table\n",
    "    if results:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"Summary of All Combinations\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"\\n{'top_k':<8} {'min_score':<10} {'Precision':<12} {'Recall':<12} {'Accuracy':<12} {'F1':<10}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for r in results:\n",
    "            print(f\"{r['top_k']:<8} {r['min_score']:<10.2f} \"\n",
    "                  f\"{r['precision']*100:>10.2f}%  {r['recall']*100:>10.2f}%  \"\n",
    "                  f\"{r['accuracy']*100:>10.2f}%  {r['f1']:>8.4f}\")\n",
    "        \n",
    "        # Find best combinations\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"Best Combinations:\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Best precision\n",
    "        best_precision = max(results, key=lambda x: x['precision'])\n",
    "        print(f\"\\nüèÜ Best Precision: top_k={best_precision['top_k']}, min_score={best_precision['min_score']}\")\n",
    "        print(f\"   Precision: {best_precision['precision']*100:.2f}%\")\n",
    "        print(f\"   Recall: {best_precision['recall']*100:.2f}%\")\n",
    "        print(f\"   Accuracy: {best_precision['accuracy']*100:.2f}%\")\n",
    "        \n",
    "        # Best F1 (balance)\n",
    "        best_f1 = max(results, key=lambda x: x['f1'])\n",
    "        print(f\"\\n‚öñÔ∏è  Best F1 Score (Balance): top_k={best_f1['top_k']}, min_score={best_f1['min_score']}\")\n",
    "        print(f\"   Precision: {best_f1['precision']*100:.2f}%\")\n",
    "        print(f\"   Recall: {best_f1['recall']*100:.2f}%\")\n",
    "        print(f\"   Accuracy: {best_f1['accuracy']*100:.2f}%\")\n",
    "        print(f\"   F1: {best_f1['f1']:.4f}\")\n",
    "        \n",
    "        # Best accuracy\n",
    "        best_accuracy = max(results, key=lambda x: x['accuracy'])\n",
    "        print(f\"\\nüéØ Best Accuracy: top_k={best_accuracy['top_k']}, min_score={best_accuracy['min_score']}\")\n",
    "        print(f\"   Precision: {best_accuracy['precision']*100:.2f}%\")\n",
    "        print(f\"   Recall: {best_accuracy['recall']*100:.2f}%\")\n",
    "        print(f\"   Accuracy: {best_accuracy['accuracy']*100:.2f}%\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"‚úì Parameter testing function ready!\")\n",
    "print(\"\\nTo test different combinations:\")\n",
    "print(\"  test_parameter_combinations()\")\n",
    "print(\"\\n‚ö†Ô∏è  Note: This will take a while as it tests multiple combinations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "336b1c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Keyword filtering analysis function ready!\n",
      "\n",
      "To analyze keyword filtering impact:\n",
      "  analyze_keyword_filtering_impact()\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Analyze Keyword Filtering Impact\n",
    "# ============================================\n",
    "# Check if ground truth ECLI contain the keywords used in filtering\n",
    "\n",
    "def analyze_keyword_filtering_impact():\n",
    "    \"\"\"\n",
    "    Analyze whether keyword filtering is helping or hurting precision.\n",
    "    Checks if ground truth ECLI contain the filtering keywords.\n",
    "    \"\"\"\n",
    "    ground_truth = load_ground_truth_ecli()\n",
    "    if not ground_truth:\n",
    "        print(\"‚ö† Cannot analyze: No ground truth data available\")\n",
    "        return None\n",
    "    \n",
    "    # Get all unique ECLI from ground truth\n",
    "    all_ground_truth_ecli = set()\n",
    "    for zaaknummer, ecli_list in ground_truth.items():\n",
    "        for ecli in ecli_list:\n",
    "            all_ground_truth_ecli.add(ecli)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"Keyword Filtering Impact Analysis\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nTotal unique ECLI in ground truth: {len(all_ground_truth_ecli)}\")\n",
    "    \n",
    "    # Keywords used in filtering\n",
    "    bicycle_keywords = [\n",
    "        \"fiets\", \"bicycle\", \"bike\", \"wiel\", \"tweewieler\",\n",
    "        \"scooter\", \"bromfiets\", \"slepen\", \"wegslepen\", \"towing\",\n",
    "        \"stalling\", \"parkeren\", \"parking\", \"verwijdering\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nKeywords used in filtering: {bicycle_keywords}\")\n",
    "    \n",
    "    # Check how many ground truth ECLI contain these keywords\n",
    "    ecli_with_keywords = {}\n",
    "    ecli_without_keywords = []\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        for ecli_number in all_ground_truth_ecli:\n",
    "            # Get all chunks for this ECLI\n",
    "            chunks = conn.execute(sqltext(\"\"\"\n",
    "                SELECT c.text FROM chunks c\n",
    "                JOIN documents d ON c.doc_id = d.doc_id\n",
    "                WHERE d.doc_type = 'ecli' AND c.doc_id = :ecli_id\n",
    "            \"\"\"), {\"ecli_id\": ecli_number}).mappings().all()\n",
    "            \n",
    "            if not chunks:\n",
    "                ecli_without_keywords.append(ecli_number)\n",
    "                continue\n",
    "            \n",
    "            # Check if any chunk contains any keyword\n",
    "            found_keywords = []\n",
    "            all_text = \" \".join([chunk['text'].lower() for chunk in chunks])\n",
    "            \n",
    "            for keyword in bicycle_keywords:\n",
    "                if keyword.lower() in all_text:\n",
    "                    found_keywords.append(keyword)\n",
    "            \n",
    "            if found_keywords:\n",
    "                ecli_with_keywords[ecli_number] = found_keywords\n",
    "            else:\n",
    "                ecli_without_keywords.append(ecli_number)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Results:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\n‚úÖ ECLI containing keywords: {len(ecli_with_keywords)} ({len(ecli_with_keywords)/len(all_ground_truth_ecli)*100:.1f}%)\")\n",
    "    print(f\"‚ùå ECLI NOT containing keywords: {len(ecli_without_keywords)} ({len(ecli_without_keywords)/len(all_ground_truth_ecli)*100:.1f}%)\")\n",
    "    \n",
    "    if ecli_without_keywords:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: {len(ecli_without_keywords)} ground truth ECLI would be filtered out!\")\n",
    "        print(f\"   Sample ECLI without keywords (first 5):\")\n",
    "        for ecli in ecli_without_keywords[:5]:\n",
    "            print(f\"     - {ecli}\")\n",
    "    \n",
    "    # Count how many advice letters would be affected\n",
    "    affected_advice = 0\n",
    "    for zaaknummer, ecli_list in ground_truth.items():\n",
    "        # Check if any expected ECLI would be filtered\n",
    "        if any(ecli in ecli_without_keywords for ecli in ecli_list):\n",
    "            affected_advice += 1\n",
    "    \n",
    "    print(f\"\\nüìä Impact on advice letters:\")\n",
    "    print(f\"   Advice letters with filtered ECLI: {affected_advice} ({affected_advice/len(ground_truth)*100:.1f}%)\")\n",
    "    \n",
    "    # Most common keywords in ground truth ECLI\n",
    "    keyword_counts = {}\n",
    "    for ecli, keywords in ecli_with_keywords.items():\n",
    "        for kw in keywords:\n",
    "            keyword_counts[kw] = keyword_counts.get(kw, 0) + 1\n",
    "    \n",
    "    if keyword_counts:\n",
    "        print(f\"\\nüìà Most common keywords in ground truth ECLI:\")\n",
    "        sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        for kw, count in sorted_keywords[:10]:\n",
    "            print(f\"   {kw}: {count} ECLI ({count/len(ecli_with_keywords)*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'ecli_with_keywords': ecli_with_keywords,\n",
    "        'ecli_without_keywords': ecli_without_keywords,\n",
    "        'affected_advice': affected_advice,\n",
    "        'keyword_counts': keyword_counts\n",
    "    }\n",
    "\n",
    "print(\"‚úì Keyword filtering analysis function ready!\")\n",
    "print(\"\\nTo analyze keyword filtering impact:\")\n",
    "print(\"  analyze_keyword_filtering_impact()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef7e654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ECLI citation finder ready!\n",
      "\n",
      "Usage:\n",
      "  results = find_relevant_ecli('your advice letter text', top_n=10)\n",
      "  print(format_ecli_citations(results))\n",
      "\n",
      "Or test with:\n",
      "  test_ecli_search('your advice letter text')\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Find Relevant ECLI Numbers for Advice Letters\n",
    "# ============================================\n",
    "# This function takes a new advice letter text and returns relevant ECLI numbers\n",
    "# that can be cited in the advice letter.\n",
    "\n",
    "def find_relevant_ecli(advice_text, top_n=10, min_score=0.3, \n",
    "                      keyword_filter=False, use_reranker=None, rerank_top_k=50,\n",
    "                      include_popular=True, popular_min_citations=100):\n",
    "    \"\"\"\n",
    "    Find relevant ECLI numbers for a given advice letter text.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    advice_text : str\n",
    "        The text content of the advice letter\n",
    "    top_n : int\n",
    "        Maximum number of ECLI numbers to return\n",
    "    min_score : float\n",
    "        Minimum hybrid score threshold (0.0 to 1.0)\n",
    "    keyword_filter : bool\n",
    "        Apply keyword filtering for bicycle-related cases (default: True)\n",
    "        Filters ECLI chunks containing bicycle-related keywords\n",
    "        This is important because ECLI data contains many non-bicycle cases,\n",
    "        while advice letters are all bicycle-related\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : List of dictionaries containing:\n",
    "        - ecli_number: The ECLI identifier\n",
    "        - score: Relevance score\n",
    "        - text_snippet: Relevant text snippet from the ECLI\n",
    "        - doc_id: Document ID\n",
    "    \"\"\"\n",
    "    # Check if ECLI documents exist\n",
    "    with engine.connect() as conn:\n",
    "        ecli_count = conn.execute(sqltext(\"\"\"\n",
    "            SELECT COUNT(*) FROM documents WHERE doc_type = 'ecli'\n",
    "        \"\"\")).scalar()\n",
    "    \n",
    "    if ecli_count == 0:\n",
    "        print(\"‚ö† No ECLI documents found in database. Please run import_all_data() first.\")\n",
    "        return []\n",
    "    \n",
    "    # Perform hybrid search - filter by doc_type='ecli' using SQL join\n",
    "    # We'll search and then filter by doc_type\n",
    "    q_emb = model.encode([advice_text], return_dense=True)[\"dense_vecs\"][0]\n",
    "    \n",
    "    # Keyword filtering for bicycle-related cases\n",
    "    # Since advice letters are all about bicycle towing, filter ECLI by keywords\n",
    "    keyword_filter_sql = \"\"\n",
    "    if keyword_filter:\n",
    "        # Dutch keywords for bicycle-related cases\n",
    "        bicycle_keywords = [\n",
    "            \"fiets\", \"bicycle\", \"bike\", \"wiel\", \"tweewieler\",\n",
    "            \"scooter\", \"bromfiets\", \"slepen\", \"wegslepen\", \"towing\",\n",
    "            \"stalling\", \"parkeren\", \"parking\", \"verwijdering\"\n",
    "        ]\n",
    "        # Create SQL filter for keyword matching\n",
    "        keyword_conditions = \" OR \".join([f\"LOWER(c.text) LIKE '%{kw}%'\" for kw in bicycle_keywords])\n",
    "        keyword_filter_sql = f\"AND ({keyword_conditions})\"\n",
    "    \n",
    "    # Dense search with doc_type filter and optional keyword filter\n",
    "    # Convert embedding to PostgreSQL vector type\n",
    "    # Determine if we're using reranker\n",
    "    if use_reranker is None:\n",
    "        use_reranker = USE_RERANKER and reranker is not None\n",
    "    \n",
    "    # Determine how many candidates to retrieve\n",
    "    # If using reranker, retrieve more candidates (rerank_top_k)\n",
    "    # Otherwise, retrieve top_n * 5 for deduplication\n",
    "    if use_reranker:\n",
    "        retrieve_k = rerank_top_k * 4  # Get more to ensure enough after deduplication\n",
    "    else:\n",
    "        retrieve_k = top_n * 5\n",
    "    \n",
    "    # pgvector requires the array to be properly cast to vector type\n",
    "    qvec_list = list(map(float, q_emb))\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        # Use PostgreSQL array syntax and cast to vector\n",
    "        # Build array literal string safely\n",
    "        array_values = ','.join(map(str, qvec_list))\n",
    "        # Execute with array literal cast to vector\n",
    "        # Get more candidates to ensure we have enough after deduplication\n",
    "        # Include section_title and doc_id for structure information\n",
    "        dense_rows = conn.execute(sqltext(f\"\"\"\n",
    "            SELECT c.chunk_id, c.doc_id, c.text, c.section_title,\n",
    "            1 - (c.embedding <=> (ARRAY[{array_values}]::vector)) AS score\n",
    "            FROM chunks c\n",
    "            JOIN documents d ON c.doc_id = d.doc_id\n",
    "            WHERE d.doc_type = 'ecli'\n",
    "            {keyword_filter_sql}\n",
    "            ORDER BY c.embedding <=> (ARRAY[{array_values}]::vector)\n",
    "            LIMIT :k\n",
    "        \"\"\"), {\"k\": retrieve_k}).mappings().all()\n",
    "    \n",
    "    dense_results = {r[\"chunk_id\"]: {\n",
    "        \"chunk_id\": r[\"chunk_id\"],\n",
    "        \"doc_id\": r[\"doc_id\"],\n",
    "        \"text\": r[\"text\"],\n",
    "        \"section_title\": r.get(\"section_title\"),  # Include section_title\n",
    "        \"score_dense\": float(r[\"score\"])\n",
    "    } for r in dense_rows}\n",
    "    \n",
    "    # BM25 search - get ECLI doc_ids first, then filter\n",
    "    with engine.connect() as conn:\n",
    "        ecli_doc_ids = [row[0] for row in conn.execute(sqltext(\"\"\"\n",
    "            SELECT doc_id FROM documents WHERE doc_type = 'ecli'\n",
    "        \"\"\")).fetchall()]\n",
    "    \n",
    "    if not ecli_doc_ids:\n",
    "        return []\n",
    "    \n",
    "    # Elasticsearch query with doc_id filter (only if Elasticsearch is available)\n",
    "    bm25_results = {}\n",
    "    if es:\n",
    "        try:\n",
    "            if es.ping():\n",
    "                es_query = {\n",
    "                    \"bool\": {\n",
    "                        \"must\": [{\"match\": {\"text\": {\"query\": advice_text}}}],\n",
    "                        \"filter\": [{\"terms\": {\"doc_id\": ecli_doc_ids}}]\n",
    "                    }\n",
    "                }\n",
    "                resp = es.search(index=ES_INDEX, query=es_query, size=top_n*3)\n",
    "                for hit in resp[\"hits\"][\"hits\"]:\n",
    "                    src = hit[\"_source\"]\n",
    "                    bm25_results[src[\"chunk_id\"]] = {\n",
    "                        \"chunk_id\": src[\"chunk_id\"],\n",
    "                        \"doc_id\": src[\"doc_id\"],\n",
    "                        \"text\": src[\"text\"],\n",
    "                        \"score_bm25\": float(hit[\"_score\"])\n",
    "                    }\n",
    "        except Exception:\n",
    "            # Elasticsearch not available, continue with dense search only\n",
    "            pass\n",
    "    \n",
    "    # Combine and normalize scores\n",
    "    all_chunks = set(dense_results.keys()) | set(bm25_results.keys())\n",
    "    \n",
    "    if not all_chunks:\n",
    "        return []\n",
    "    \n",
    "    # Get raw scores (before normalization)\n",
    "    dense_scores_raw = [dense_results.get(k, {}).get(\"score_dense\") for k in all_chunks]\n",
    "    bm25_scores_raw = [bm25_results.get(k, {}).get(\"score_bm25\") for k in all_chunks]\n",
    "    \n",
    "    # Normalize scores separately for dense and BM25\n",
    "    # Use minmax normalization but preserve relative differences\n",
    "    def minmax(scores):\n",
    "        if not scores: return scores\n",
    "        vals = [s for s in scores if s is not None]\n",
    "        if not vals: return [0.0] * len(scores)\n",
    "        lo, hi = min(vals), max(vals)\n",
    "        if hi == lo:\n",
    "            # If all scores are the same, return original scores (not all 1.0)\n",
    "            return [float(s) if s is not None else 0.0 for s in scores]\n",
    "        return [( (s - lo) / (hi - lo) ) if s is not None else 0.0 for s in scores]\n",
    "    \n",
    "    # Normalize dense scores\n",
    "    dn = minmax(dense_scores_raw)\n",
    "    \n",
    "    # Normalize BM25 scores (if available)\n",
    "    if any(s is not None for s in bm25_scores_raw):\n",
    "        bn = minmax(bm25_scores_raw)\n",
    "    else:\n",
    "        # If no BM25 results, use zeros\n",
    "        bn = [0.0] * len(all_chunks)\n",
    "    \n",
    "    # Combine results with weighted scores\n",
    "    # Use original dense scores for better differentiation\n",
    "    combined = []\n",
    "    for i, cid in enumerate(all_chunks):\n",
    "        # Use normalized scores for combination\n",
    "        sd = dn[i] if dn[i] is not None else 0.0\n",
    "        sb = bn[i] if bn[i] is not None else 0.0\n",
    "        \n",
    "        # Hybrid score: 70% dense (semantic), 30% BM25 (keyword)\n",
    "        # If BM25 is not available, use 100% dense\n",
    "        if any(s is not None for s in bm25_scores_raw):\n",
    "            score = 0.7 * sd + 0.3 * sb\n",
    "        else:\n",
    "            score = sd  # Use only dense score if BM25 unavailable\n",
    "        \n",
    "        rec = dense_results.get(cid) or bm25_results.get(cid)\n",
    "        rec[\"score_hybrid\"] = float(score)\n",
    "        rec[\"score_dense_raw\"] = float(dense_scores_raw[i]) if dense_scores_raw[i] is not None else 0.0\n",
    "        combined.append(rec)\n",
    "    \n",
    "    combined.sort(key=lambda x: x[\"score_hybrid\"], reverse=True)\n",
    "    \n",
    "    # Filter by threshold and deduplicate by ECLI number\n",
    "    # Note: We don't debias popular ECLI because if they are truly relevant,\n",
    "    # they should be selected. The low precision (7.18%) suggests the system\n",
    "    # is NOT correctly identifying relevant ECLI (including popular ones),\n",
    "    # so we should focus on improving relevance scoring, not penalizing popular ones.\n",
    "    # Improved filtering: use raw dense score for better quality control\n",
    "    ecli_results = []\n",
    "    seen_ecli = set()\n",
    "    \n",
    "    # Determine how many candidates to collect\n",
    "    # If using reranker, collect more candidates (rerank_top_k)\n",
    "    # Otherwise, collect top_n\n",
    "    if use_reranker:\n",
    "        collect_k = rerank_top_k\n",
    "    else:\n",
    "        collect_k = top_n\n",
    "    \n",
    "    # Use adaptive dense threshold based on min_score\n",
    "    # If min_score is low, use stricter dense score filtering\n",
    "    # Raw dense score is cosine similarity (0-1), where 1 = identical\n",
    "    if min_score < 0.3:\n",
    "        # For low min_score, require higher raw dense similarity\n",
    "        dense_threshold = 0.6\n",
    "    elif min_score < 0.4:\n",
    "        dense_threshold = 0.55\n",
    "    else:\n",
    "        # For high min_score, hybrid score is already strict\n",
    "        dense_threshold = 0.5\n",
    "    \n",
    "    for hit in combined:\n",
    "        hybrid_score = hit['score_hybrid']\n",
    "        dense_raw = hit.get('score_dense_raw', 0.0)\n",
    "        \n",
    "        # Apply filtering: hybrid score must pass, AND raw dense score should be reasonable\n",
    "        # This helps filter out false positives from normalization artifacts\n",
    "        if hybrid_score >= min_score:\n",
    "            # Additional quality check: raw dense score should indicate some similarity\n",
    "            # But don't be too strict if we have very few results\n",
    "            if dense_raw >= dense_threshold or len(ecli_results) < 3:\n",
    "                doc_id = hit.get('doc_id')\n",
    "                if doc_id not in seen_ecli:\n",
    "                    seen_ecli.add(doc_id)\n",
    "                    # Prepare full chunk text with structure information for reranking\n",
    "                    # Use full text (or at least 1000-2000 chars) instead of just 300\n",
    "                    chunk_text_full = hit['text']\n",
    "                    chunk_text_for_rerank = chunk_text_full[:2000] if len(chunk_text_full) > 2000 else chunk_text_full\n",
    "                    \n",
    "                    # Build structured text with metadata\n",
    "                    structure_parts = []\n",
    "                    if doc_id:  # ECLI number\n",
    "                        structure_parts.append(f\"ECLI: {doc_id}\")\n",
    "                    if hit.get('section_title'):\n",
    "                        structure_parts.append(f\"Section: {hit['section_title']}\")\n",
    "                    \n",
    "                    # Combine structure info with chunk text\n",
    "                    if structure_parts:\n",
    "                        structured_text = \" | \".join(structure_parts) + \"\\n\\n\" + chunk_text_for_rerank\n",
    "                    else:\n",
    "                        structured_text = chunk_text_for_rerank\n",
    "                    \n",
    "                    ecli_results.append({\n",
    "                        'ecli_number': doc_id,  # doc_id is the ECLI number\n",
    "                        'score': hybrid_score,\n",
    "                        'score_dense_raw': dense_raw,  # Include for debugging\n",
    "                        'text_snippet': hit['text'][:300],  # Keep short snippet for display\n",
    "                        'text_full': structured_text,  # Full structured text for reranking\n",
    "                        'doc_id': doc_id,\n",
    "                        'chunk_id': hit.get('chunk_id'),\n",
    "                        'section_title': hit.get('section_title')\n",
    "                    })\n",
    "                    if len(ecli_results) >= collect_k:\n",
    "                        break\n",
    "    \n",
    "    # Apply reranking if enabled\n",
    "    if use_reranker and reranker is not None:\n",
    "        # Determine how many candidates to rerank\n",
    "        # If we have fewer results than rerank_top_k, use all available\n",
    "        candidates_to_rerank = min(rerank_top_k, len(ecli_results))\n",
    "        \n",
    "        if candidates_to_rerank > top_n:\n",
    "            # Get candidates for reranking\n",
    "            candidates_for_rerank = ecli_results[:candidates_to_rerank]\n",
    "            \n",
    "            # Prepare query-document pairs for reranking\n",
    "            # Use full structured text (1000-2000 chars) instead of just 300 chars\n",
    "            pairs = []\n",
    "            for candidate in candidates_for_rerank:\n",
    "                # Use text_full (structured, 1000-2000 chars) if available, otherwise use text_snippet\n",
    "                doc_text = candidate.get('text_full', candidate.get('text_snippet', ''))\n",
    "                pairs.append([advice_text, doc_text])\n",
    "            \n",
    "            # Rerank the candidates\n",
    "            try:\n",
    "                rerank_scores = reranker.compute_score(pairs, normalize=True)\n",
    "                \n",
    "                # Ensure rerank_scores is a list\n",
    "                if not isinstance(rerank_scores, list):\n",
    "                    rerank_scores = [rerank_scores] if len(candidates_for_rerank) == 1 else list(rerank_scores)\n",
    "                \n",
    "                # Debug: Print reranker scores (only for first call to avoid spam)\n",
    "                if not hasattr(find_relevant_ecli, '_rerank_debug_printed'):\n",
    "                    print(f\"üîç Reranker Debug: Reranking {len(candidates_for_rerank)} candidates\")\n",
    "                    original_scores = [f\"{c['score']:.4f}\" for c in candidates_for_rerank[:3]]\n",
    "                    print(f\"   Original top 3 scores: {original_scores}\")\n",
    "                    find_relevant_ecli._rerank_debug_printed = True\n",
    "                \n",
    "                # Update scores with reranker scores\n",
    "                for i, candidate in enumerate(candidates_for_rerank):\n",
    "                    # Combine original score with reranker score\n",
    "                    # Weight: 20% original, 80% reranker (reranker is more accurate)\n",
    "                    original_score = candidate['score']\n",
    "                    rerank_score = float(rerank_scores[i])\n",
    "                    candidate['score'] = 0.2 * original_score + 0.8 * rerank_score\n",
    "                    candidate['score_rerank'] = rerank_score\n",
    "                    candidate['score_original'] = original_score\n",
    "                \n",
    "                # Re-sort by new combined score\n",
    "                candidates_for_rerank.sort(key=lambda x: x['score'], reverse=True)\n",
    "                \n",
    "                # Debug: Print reranked top 3 scores\n",
    "                if not hasattr(find_relevant_ecli, '_rerank_debug_printed2'):\n",
    "                    reranked_scores = [f\"{c['score']:.4f}\" for c in candidates_for_rerank[:3]]\n",
    "                    print(f\"   Reranked top 3 scores: {reranked_scores}\")\n",
    "                    find_relevant_ecli._rerank_debug_printed2 = True\n",
    "                \n",
    "                # Take top_n after reranking\n",
    "                ecli_results = candidates_for_rerank[:top_n]\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† Reranking failed: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                print(\"   Using original results without reranking...\")\n",
    "                # Keep original results if reranking fails\n",
    "                ecli_results = ecli_results[:top_n]\n",
    "        else:\n",
    "            # Not enough candidates to rerank, just take top_n\n",
    "            ecli_results = ecli_results[:top_n]\n",
    "    else:\n",
    "        # No reranking, just take top_n\n",
    "        if use_reranker and reranker is None:\n",
    "            print(\"‚ö† Reranker requested but not loaded. Check Cell 4.\")\n",
    "        ecli_results = ecli_results[:top_n]\n",
    "    \n",
    "    # Add popular ECLI if enabled\n",
    "    if include_popular:\n",
    "        try:\n",
    "            popular_ecli_list = get_popular_ecli(min_citations=popular_min_citations)\n",
    "            \n",
    "            if popular_ecli_list:\n",
    "                # Get query embedding for similarity calculation\n",
    "                q_emb = model.encode([advice_text], return_dense=True)[\"dense_vecs\"][0]\n",
    "                qvec_list = list(map(float, q_emb))\n",
    "                array_values = ','.join(map(str, qvec_list))\n",
    "                \n",
    "                # Get existing ECLI numbers to avoid duplicates\n",
    "                existing_ecli = set([r['ecli_number'] for r in ecli_results])\n",
    "                \n",
    "                # Calculate similarity scores for popular ECLI\n",
    "                with engine.connect() as conn:\n",
    "                    for ecli_number, citation_count in popular_ecli_list:\n",
    "                        # Skip if already in results\n",
    "                        if ecli_number in existing_ecli:\n",
    "                            continue\n",
    "                        \n",
    "                        # Get the best chunk for this ECLI and calculate similarity\n",
    "                        # Include section_title for structure information\n",
    "                        chunk_row = conn.execute(sqltext(f'''\n",
    "                            SELECT c.chunk_id, c.doc_id, c.text, c.section_title,\n",
    "                            1 - (c.embedding <=> (ARRAY[{array_values}]::vector)) AS score\n",
    "                            FROM chunks c\n",
    "                            JOIN documents d ON c.doc_id = d.doc_id\n",
    "                            WHERE d.doc_type = 'ecli' AND c.doc_id = :ecli_id\n",
    "                            ORDER BY c.embedding <=> (ARRAY[{array_values}]::vector)\n",
    "                            LIMIT 1\n",
    "                        '''), {\"ecli_id\": ecli_number}).mappings().first()\n",
    "                        \n",
    "                        if chunk_row:\n",
    "                            score = float(chunk_row['score'])\n",
    "                            \n",
    "                            # Prepare full chunk text with structure information\n",
    "                            chunk_text_full = chunk_row['text']\n",
    "                            chunk_text_for_rerank = chunk_text_full[:2000] if len(chunk_text_full) > 2000 else chunk_text_full\n",
    "                            \n",
    "                            # Build structured text with metadata\n",
    "                            structure_parts = []\n",
    "                            if ecli_number:  # ECLI number\n",
    "                                structure_parts.append(f\"ECLI: {ecli_number}\")\n",
    "                            if chunk_row.get('section_title'):\n",
    "                                structure_parts.append(f\"Section: {chunk_row['section_title']}\")\n",
    "                            \n",
    "                            # Combine structure info with chunk text\n",
    "                            if structure_parts:\n",
    "                                structured_text = \" | \".join(structure_parts) + \"\\n\\n\" + chunk_text_for_rerank\n",
    "                            else:\n",
    "                                structured_text = chunk_text_for_rerank\n",
    "                            \n",
    "                            # Add popular ECLI with its calculated score\n",
    "                            ecli_results.append({\n",
    "                                'ecli_number': ecli_number,\n",
    "                                'score': score,\n",
    "                                'score_dense_raw': score,\n",
    "                                'text_snippet': chunk_row['text'][:300],  # Keep short snippet for display\n",
    "                                'text_full': structured_text,  # Full structured text for reranking\n",
    "                                'doc_id': ecli_number,\n",
    "                                'chunk_id': chunk_row['chunk_id'],\n",
    "                                'section_title': chunk_row.get('section_title'),\n",
    "                                'is_popular': True,  # Mark as popular ECLI\n",
    "                                'citation_count': citation_count  # Include citation count\n",
    "                            })\n",
    "                            existing_ecli.add(ecli_number)\n",
    "                \n",
    "                # Re-sort all results by score (including popular ECLI)\n",
    "                # Give popular ECLI a boost to ensure they're included\n",
    "                # Sort by: is_popular first (True before False), then by score\n",
    "                ecli_results.sort(key=lambda x: (not x.get('is_popular', False), -x['score']))\n",
    "                \n",
    "                # Ensure popular ECLI are included even if they have lower scores\n",
    "                # Separate popular and non-popular results\n",
    "                popular_results = [r for r in ecli_results if r.get('is_popular', False)]\n",
    "                non_popular_results = [r for r in ecli_results if not r.get('is_popular', False)]\n",
    "                \n",
    "                # Take top non-popular results, then add popular results\n",
    "                # This ensures popular ECLI are always included if they exist\n",
    "                num_non_popular = max(0, top_n - len(popular_results))\n",
    "                final_results = non_popular_results[:num_non_popular] + popular_results\n",
    "                \n",
    "                # Re-sort by score for final output\n",
    "                final_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "                ecli_results = final_results[:top_n]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Failed to add popular ECLI: {e}\")\n",
    "            # Continue without popular ECLI if there's an error\n",
    "    \n",
    "    return ecli_results\n",
    "\n",
    "def split_advice_into_issues(advice_text, num_issues=5):\n",
    "    \"\"\"\n",
    "    Split an advice letter into multiple issues/claims for more granular retrieval.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    advice_text : str\n",
    "        The full advice letter text\n",
    "    num_issues : int\n",
    "        Number of issues to split into (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : List of issue texts (each is a portion of the advice letter)\n",
    "    \"\"\"\n",
    "    # Use spaCy to split into sentences\n",
    "    doc = nlp(advice_text)\n",
    "    sentences = [s.text.strip() for s in doc.sents if s.text.strip()]\n",
    "    \n",
    "    if len(sentences) <= num_issues:\n",
    "        # If we have fewer sentences than requested issues, return each sentence\n",
    "        return sentences\n",
    "    \n",
    "    # Split sentences into roughly equal groups\n",
    "    sentences_per_issue = len(sentences) // num_issues\n",
    "    issues = []\n",
    "    \n",
    "    for i in range(num_issues):\n",
    "        start_idx = i * sentences_per_issue\n",
    "        if i == num_issues - 1:\n",
    "            # Last issue gets all remaining sentences\n",
    "            end_idx = len(sentences)\n",
    "        else:\n",
    "            end_idx = (i + 1) * sentences_per_issue\n",
    "        \n",
    "        issue_text = \" \".join(sentences[start_idx:end_idx])\n",
    "        if issue_text.strip():\n",
    "            issues.append(issue_text.strip())\n",
    "    \n",
    "    return issues\n",
    "\n",
    "def find_relevant_ecli_by_issues(advice_text, top_n=5, num_issues=5, \n",
    "                                 retrieve_k=200, rerank_k=50, min_score=0.3):\n",
    "    \"\"\"\n",
    "    Find relevant ECLI numbers by splitting advice letter into issues and retrieving separately.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Split advice letter into num_issues issues/claims\n",
    "    2. For each issue: Retrieve@retrieve_k, Rerank@rerank_k\n",
    "    3. Merge all results\n",
    "    4. Aggregate by ECLI (select best chunk for each ECLI)\n",
    "    5. Return top_n ECLI with best evidence chunks\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    advice_text : str\n",
    "        The full advice letter text\n",
    "    top_n : int\n",
    "        Number of ECLI numbers to return (default: 5)\n",
    "    num_issues : int\n",
    "        Number of issues to split the advice letter into (default: 5)\n",
    "    retrieve_k : int\n",
    "        Number of candidates to retrieve per issue (default: 200)\n",
    "    rerank_k : int\n",
    "        Number of candidates to rerank per issue (default: 50)\n",
    "    min_score : float\n",
    "        Minimum relevance score threshold\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : List of ECLI results, each with best evidence chunk\n",
    "    \"\"\"\n",
    "    # Split advice letter into issues\n",
    "    issues = split_advice_into_issues(advice_text, num_issues=num_issues)\n",
    "    \n",
    "    if not issues:\n",
    "        return []\n",
    "    \n",
    "    # Collect all chunks from all issues\n",
    "    all_chunks = []  # List of (chunk_info, score, issue_idx)\n",
    "    \n",
    "    # Process each issue\n",
    "    for issue_idx, issue_text in enumerate(issues):\n",
    "        # Get embedding for this issue\n",
    "        q_emb = model.encode([issue_text], return_dense=True)[\"dense_vecs\"][0]\n",
    "        qvec_list = list(map(float, q_emb))\n",
    "        array_values = ','.join(map(str, qvec_list))\n",
    "        \n",
    "        with engine.connect() as conn:\n",
    "            # Retrieve candidates for this issue\n",
    "            dense_rows = conn.execute(sqltext(f\"\"\"\n",
    "                SELECT c.chunk_id, c.doc_id, c.text, c.section_title,\n",
    "                1 - (c.embedding <=> (ARRAY[{array_values}]::vector)) AS score\n",
    "                FROM chunks c\n",
    "                JOIN documents d ON c.doc_id = d.doc_id\n",
    "                WHERE d.doc_type = 'ecli'\n",
    "                ORDER BY c.embedding <=> (ARRAY[{array_values}]::vector)\n",
    "                LIMIT :k\n",
    "            \"\"\"), {\"k\": retrieve_k}).mappings().all()\n",
    "            \n",
    "            # Collect chunks with scores\n",
    "            for row in dense_rows:\n",
    "                chunk_info = {\n",
    "                    'chunk_id': row['chunk_id'],\n",
    "                    'doc_id': row['doc_id'],\n",
    "                    'text': row['text'],\n",
    "                    'section_title': row.get('section_title'),\n",
    "                    'score_dense': float(row['score']),\n",
    "                    'issue_idx': issue_idx\n",
    "                }\n",
    "                all_chunks.append((chunk_info, float(row['score']), issue_idx))\n",
    "    \n",
    "    if not all_chunks:\n",
    "        return []\n",
    "    \n",
    "    # Rerank: Group chunks by issue and rerank each issue's top rerank_k chunks\n",
    "    reranked_chunks = []\n",
    "    \n",
    "    for issue_idx in range(len(issues)):\n",
    "        # Get top rerank_k chunks for this issue\n",
    "        issue_chunks = [(chunk, score, idx) for chunk, score, idx in all_chunks if idx == issue_idx]\n",
    "        issue_chunks.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_issue_chunks = issue_chunks[:rerank_k]\n",
    "        \n",
    "        if not top_issue_chunks:\n",
    "            continue\n",
    "        \n",
    "        # Prepare pairs for reranking\n",
    "        issue_text = issues[issue_idx]\n",
    "        pairs = []\n",
    "        for chunk_info, score, _ in top_issue_chunks:\n",
    "            # Use full structured text for reranking\n",
    "            chunk_text_full = chunk_info['text']\n",
    "            chunk_text_for_rerank = chunk_text_full[:2000] if len(chunk_text_full) > 2000 else chunk_text_full\n",
    "            \n",
    "            structure_parts = []\n",
    "            if chunk_info['doc_id']:\n",
    "                structure_parts.append(f\"ECLI: {chunk_info['doc_id']}\")\n",
    "            if chunk_info.get('section_title'):\n",
    "                structure_parts.append(f\"Section: {chunk_info['section_title']}\")\n",
    "            \n",
    "            if structure_parts:\n",
    "                structured_text = \" | \".join(structure_parts) + \"\\n\\n\" + chunk_text_for_rerank\n",
    "            else:\n",
    "                structured_text = chunk_text_for_rerank\n",
    "            \n",
    "            pairs.append([issue_text, structured_text])\n",
    "        \n",
    "        # Rerank\n",
    "        if reranker is not None and USE_RERANKER:\n",
    "            try:\n",
    "                rerank_scores = reranker.compute_score(pairs, normalize=True)\n",
    "                if not isinstance(rerank_scores, list):\n",
    "                    rerank_scores = [rerank_scores] if len(top_issue_chunks) == 1 else list(rerank_scores)\n",
    "                \n",
    "                # Update scores\n",
    "                for i, (chunk_info, original_score, _) in enumerate(top_issue_chunks):\n",
    "                    rerank_score = float(rerank_scores[i])\n",
    "                    # Combine: 20% original, 80% reranker\n",
    "                    final_score = 0.2 * original_score + 0.8 * rerank_score\n",
    "                    reranked_chunks.append((chunk_info, final_score, issue_idx))\n",
    "            except Exception as e:\n",
    "                # If reranking fails, use original scores\n",
    "                for chunk_info, score, idx in top_issue_chunks:\n",
    "                    reranked_chunks.append((chunk_info, score, idx))\n",
    "        else:\n",
    "            # No reranker, use original scores\n",
    "            for chunk_info, score, idx in top_issue_chunks:\n",
    "                reranked_chunks.append((chunk_info, score, idx))\n",
    "    \n",
    "    # Aggregate by ECLI: for each ECLI, keep the best chunk\n",
    "    ecli_best_chunks = {}  # ecli_number -> (chunk_info, best_score)\n",
    "    \n",
    "    for chunk_info, score, issue_idx in reranked_chunks:\n",
    "        if score < min_score:\n",
    "            continue\n",
    "        \n",
    "        ecli_number = chunk_info['doc_id']\n",
    "        \n",
    "        if ecli_number not in ecli_best_chunks:\n",
    "            ecli_best_chunks[ecli_number] = (chunk_info, score, issue_idx)\n",
    "        else:\n",
    "            # Keep the chunk with higher score\n",
    "            current_score = ecli_best_chunks[ecli_number][1]\n",
    "            if score > current_score:\n",
    "                ecli_best_chunks[ecli_number] = (chunk_info, score, issue_idx)\n",
    "    \n",
    "    # Convert to result format and sort by score\n",
    "    results = []\n",
    "    for ecli_number, (chunk_info, score, issue_idx) in ecli_best_chunks.items():\n",
    "        # Prepare structured text\n",
    "        chunk_text_full = chunk_info['text']\n",
    "        chunk_text_for_rerank = chunk_text_full[:2000] if len(chunk_text_full) > 2000 else chunk_text_full\n",
    "        \n",
    "        structure_parts = []\n",
    "        if ecli_number:\n",
    "            structure_parts.append(f\"ECLI: {ecli_number}\")\n",
    "        if chunk_info.get('section_title'):\n",
    "            structure_parts.append(f\"Section: {chunk_info['section_title']}\")\n",
    "        \n",
    "        if structure_parts:\n",
    "            structured_text = \" | \".join(structure_parts) + \"\\n\\n\" + chunk_text_for_rerank\n",
    "        else:\n",
    "            structured_text = chunk_text_for_rerank\n",
    "        \n",
    "        results.append({\n",
    "            'ecli_number': ecli_number,\n",
    "            'score': score,\n",
    "            'text_snippet': chunk_info['text'][:300],\n",
    "            'text_full': structured_text,\n",
    "            'doc_id': ecli_number,\n",
    "            'chunk_id': chunk_info['chunk_id'],\n",
    "            'section_title': chunk_info.get('section_title'),\n",
    "            'best_evidence_chunk': chunk_info['text'],  # Full chunk text as evidence\n",
    "            'issue_idx': issue_idx  # Which issue this chunk came from\n",
    "        })\n",
    "    \n",
    "    # Sort by score and return top_n\n",
    "    results.sort(key=lambda x: x['score'], reverse=True)\n",
    "    return results[:top_n]\n",
    "\n",
    "def format_ecli_citations(ecli_results, show_evidence=False):\n",
    "    \"\"\"\n",
    "    Format ECLI results as a readable citation list.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ecli_results : list\n",
    "        Results from find_relevant_ecli() or find_relevant_ecli_by_issues()\n",
    "    show_evidence : bool\n",
    "        If True, show best evidence chunk (for issues-based retrieval)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Formatted string with citations\n",
    "    \"\"\"\n",
    "    if not ecli_results:\n",
    "        return \"No relevant ECLI numbers found.\"\n",
    "    \n",
    "    output = []\n",
    "    output.append(f\"\\n{'='*70}\")\n",
    "    output.append(f\"Found {len(ecli_results)} relevant ECLI numbers:\")\n",
    "    output.append(f\"{'='*70}\\n\")\n",
    "    \n",
    "    for i, result in enumerate(ecli_results, 1):\n",
    "        output.append(f\"{i}. {result['ecli_number']}\")\n",
    "        output.append(f\"   Relevance Score: {result['score']:.4f}\")\n",
    "        \n",
    "        # Show which issue this came from (if available)\n",
    "        if 'issue_idx' in result:\n",
    "            output.append(f\"   From Issue: {result['issue_idx'] + 1}/5\")\n",
    "        \n",
    "        # Show best evidence chunk if available\n",
    "        if show_evidence and 'best_evidence_chunk' in result:\n",
    "            evidence = result['best_evidence_chunk']\n",
    "            output.append(f\"   Best Evidence Chunk:\")\n",
    "            output.append(f\"   {evidence[:500]}...\")\n",
    "        else:\n",
    "            output.append(f\"   Snippet: {result['text_snippet'][:200]}...\")\n",
    "        \n",
    "        output.append(\"\")\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "# Example usage function\n",
    "def test_ecli_search(advice_text_sample=None):\n",
    "    \"\"\"\n",
    "    Test the ECLI search functionality with a sample advice letter.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    advice_text_sample : str, optional\n",
    "        Sample advice letter text. If None, uses a default example.\n",
    "    \"\"\"\n",
    "    if advice_text_sample is None:\n",
    "        # Default test query\n",
    "        advice_text_sample = \"bezwaar tegen wegnemen fiets parkeerverbod\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"Testing ECLI Citation Finder\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nQuery (Advice Letter Text):\")\n",
    "    print(f\"{advice_text_sample[:200]}...\")\n",
    "    \n",
    "    results = find_relevant_ecli(advice_text_sample, top_n=10, min_score=0.3)\n",
    "    \n",
    "    if results:\n",
    "        print(format_ecli_citations(results))\n",
    "        return results\n",
    "    else:\n",
    "        print(\"\\n‚ö† No relevant ECLI numbers found.\")\n",
    "        print(\"   This could mean:\")\n",
    "        print(\"   1. No ECLI data has been imported yet (run import_all_data())\")\n",
    "        print(\"   2. The query doesn't match any ECLI content\")\n",
    "        print(\"   3. Try lowering the min_score threshold\")\n",
    "        return []\n",
    "\n",
    "print(\"‚úì ECLI citation finder ready!\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  results = find_relevant_ecli('your advice letter text', top_n=10)\")\n",
    "print(\"  print(format_ecli_citations(results))\")\n",
    "print(\"\\nOr test with:\")\n",
    "print(\"  test_ecli_search('your advice letter text')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cbc0546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Database check function ready!\n",
      "\n",
      "To check if data has been imported, run:\n",
      "  check_database_data()\n",
      "\n",
      "======================================================================\n",
      "üí° Important: Data Persistence\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Data stored in PostgreSQL is PERSISTENT:\n",
      "   - Restarting kernel ‚Üí Data remains ‚úì\n",
      "   - Closing notebook ‚Üí Data remains ‚úì\n",
      "   - Restarting computer ‚Üí Data remains ‚úì (if PostgreSQL is running)\n",
      "\n",
      "‚ö†Ô∏è  However, restarting kernel will:\n",
      "   - Clear memory variables (engine, model, etc.)\n",
      "   - Require re-running Cell 2 (Database Setup)\n",
      "   - Require re-running Cell 4 (Model Loading)\n",
      "\n",
      "üíæ To check if data exists after kernel restart:\n",
      "   1. Run Cell 2 (Database Setup) to reconnect\n",
      "   2. Run check_database_data() to verify data\n",
      "   3. If data exists, skip Cell 6 (import_all_data)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Check Database Data Status\n",
    "# ============================================\n",
    "# Use this function to verify if data has been imported\n",
    "\n",
    "def check_database_data():\n",
    "    \"\"\"\n",
    "    Check if data has been imported into the database.\n",
    "    Returns a summary of what's in the database.\n",
    "    \n",
    "    This function helps verify whether import_all_data() has been run.\n",
    "    \"\"\"\n",
    "    if engine is None:\n",
    "        print(\"‚úó Database engine is None - cannot check data\")\n",
    "        print(\"   Please run Cell 2 (Database Setup) first\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            # Check document counts\n",
    "            ecli_count = conn.execute(sqltext(\"\"\"\n",
    "                SELECT COUNT(*) FROM documents WHERE doc_type = 'ecli'\n",
    "            \"\"\")).scalar()\n",
    "            \n",
    "            advice_count = conn.execute(sqltext(\"\"\"\n",
    "                SELECT COUNT(*) FROM documents WHERE doc_type = 'advice'\n",
    "            \"\"\")).scalar()\n",
    "            \n",
    "            chunk_count = conn.execute(sqltext(\"\"\"\n",
    "                SELECT COUNT(*) FROM chunks\n",
    "            \"\"\")).scalar()\n",
    "            \n",
    "            # Check if there are any embeddings\n",
    "            embedding_count = conn.execute(sqltext(\"\"\"\n",
    "                SELECT COUNT(*) FROM chunks WHERE embedding IS NOT NULL\n",
    "            \"\"\")).scalar()\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"üìä Database Data Status\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nDocuments:\")\n",
    "        print(f\"  ECLI documents: {ecli_count}\")\n",
    "        print(f\"  Advice documents: {advice_count}\")\n",
    "        print(f\"  Total documents: {ecli_count + advice_count}\")\n",
    "        print(f\"\\nChunks:\")\n",
    "        print(f\"  Total chunks: {chunk_count}\")\n",
    "        print(f\"  Chunks with embeddings: {embedding_count}\")\n",
    "        \n",
    "        if ecli_count == 0 and advice_count == 0:\n",
    "            print(\"\\n‚ö†Ô∏è  No data found in database!\")\n",
    "            print(\"   You need to run Cell 6 to import data:\")\n",
    "            print(\"   import_all_data()\")\n",
    "        elif ecli_count > 0 and advice_count > 0:\n",
    "            print(\"\\n‚úÖ Data has been imported!\")\n",
    "            print(f\"   You can now use:\")\n",
    "            print(f\"   - find_relevant_ecli() to search for ECLI\")\n",
    "            print(f\"   - evaluate_all_advice_letters() to evaluate the system\")\n",
    "            print(f\"   - test_with_existing_advice_letter() to test with existing data\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  Partial data found:\")\n",
    "            if ecli_count == 0:\n",
    "                print(\"   - Missing ECLI documents\")\n",
    "            if advice_count == 0:\n",
    "                print(\"   - Missing Advice documents\")\n",
    "            print(\"   Run Cell 6 to import missing data: import_all_data()\")\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return {\n",
    "            'ecli_count': ecli_count,\n",
    "            'advice_count': advice_count,\n",
    "            'chunk_count': chunk_count,\n",
    "            'embedding_count': embedding_count,\n",
    "            'has_data': ecli_count > 0 and advice_count > 0\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error checking database: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úì Database check function ready!\")\n",
    "print(\"\\nTo check if data has been imported, run:\")\n",
    "print(\"  check_database_data()\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üí° Important: Data Persistence\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Data stored in PostgreSQL is PERSISTENT:\")\n",
    "print(\"   - Restarting kernel ‚Üí Data remains ‚úì\")\n",
    "print(\"   - Closing notebook ‚Üí Data remains ‚úì\")\n",
    "print(\"   - Restarting computer ‚Üí Data remains ‚úì (if PostgreSQL is running)\")\n",
    "print(\"\\n‚ö†Ô∏è  However, restarting kernel will:\")\n",
    "print(\"   - Clear memory variables (engine, model, etc.)\")\n",
    "print(\"   - Require re-running Cell 2 (Database Setup)\")\n",
    "print(\"   - Require re-running Cell 4 (Model Loading)\")\n",
    "print(\"\\nüíæ To check if data exists after kernel restart:\")\n",
    "print(\"   1. Run Cell 2 (Database Setup) to reconnect\")\n",
    "print(\"   2. Run check_database_data() to verify data\")\n",
    "print(\"   3. If data exists, skip Cell 6 (import_all_data)\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac8c92c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ECLI Citation Finder - Ready to Use\n",
      "======================================================================\n",
      "\n",
      "Option 1: Test with existing advice letter from database\n",
      "  test_with_existing_advice_letter()  # Random selection\n",
      "  test_with_existing_advice_letter(advice_index=0)  # Specific index\n",
      "\n",
      "Option 2: Use Word document\n",
      "  docx_path = 'your_advice_letter.docx'\n",
      "  advice_text = read_word_document(docx_path)\n",
      "  results = find_relevant_ecli(advice_text, top_n=10)\n",
      "  print(format_ecli_citations(results))\n",
      "\n",
      "Option 3: Use text string\n",
      "  results = find_relevant_ecli('your advice text', top_n=10)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Find ECLI Numbers for Advice Letters\n",
    "# ============================================\n",
    "# This cell demonstrates how to use the RAG system to find relevant ECLI numbers\n",
    "# for a given advice letter text, including reading from Word documents (.docx)\n",
    "\n",
    "# Note: This cell assumes you have already:\n",
    "# 1. Run Cell 2 (Database Setup)\n",
    "# 2. Run Cell 7 (import_all_data) to import ECLI data\n",
    "# 3. The find_relevant_ecli() function is defined in Cell 9\n",
    "\n",
    "# Install python-docx if needed (uncomment to install)\n",
    "# import subprocess\n",
    "# import sys\n",
    "# subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"python-docx\"])\n",
    "\n",
    "# Function to read text from Word document\n",
    "def read_word_document(docx_path):\n",
    "    \"\"\"\n",
    "    Read text content from a Word document (.docx file).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    docx_path : str\n",
    "        Path to the .docx file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Extracted text content from the document\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from docx import Document\n",
    "    except ImportError:\n",
    "        print(\"‚úó python-docx not installed. Installing...\")\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"python-docx\"])\n",
    "        from docx import Document\n",
    "    \n",
    "    doc = Document(docx_path)\n",
    "    # Extract text from all paragraphs\n",
    "    text_parts = [paragraph.text for paragraph in doc.paragraphs]\n",
    "    # Join all paragraphs with newlines\n",
    "    full_text = \"\\n\".join(text_parts)\n",
    "    return full_text\n",
    "\n",
    "# ============================================\n",
    "# Examples: How to use with Word documents\n",
    "# ============================================\n",
    "\n",
    "# Example 1: Read from Word document and find ECLI numbers\n",
    "# --------------------------------------------------------\n",
    "# docx_path = \"path/to/your/advice_letter.docx\"\n",
    "# advice_text = read_word_document(docx_path)\n",
    "# results = find_relevant_ecli(advice_text, top_n=10, min_score=0.3)\n",
    "# print(format_ecli_citations(results))\n",
    "\n",
    "# Example 2: Get just the ECLI numbers as a list\n",
    "# ----------------------------------------------\n",
    "# docx_path = \"path/to/your/advice_letter.docx\"\n",
    "# advice_text = read_word_document(docx_path)\n",
    "# results = find_relevant_ecli(advice_text, top_n=10, min_score=0.3)\n",
    "# ecli_numbers = [r['ecli_number'] for r in results]\n",
    "# print(f\"Found {len(ecli_numbers)} relevant ECLI numbers:\")\n",
    "# for ecli in ecli_numbers:\n",
    "#     print(f\"  - {ecli}\")\n",
    "\n",
    "# Example 3: Simple text string (if you already have the text)\n",
    "# ------------------------------------------------------------\n",
    "# advice_text = \"\"\"\n",
    "# Your complete advice letter text here...\n",
    "# This can be the full content of the advice letter.\n",
    "# \"\"\"\n",
    "# results = find_relevant_ecli(advice_text, top_n=10, min_score=0.3)\n",
    "# print(format_ecli_citations(results))\n",
    "\n",
    "def diagnose_search_issue(advice_indices=[0, 10, 50]):\n",
    "    \"\"\"\n",
    "    Diagnose why different advice letters return the same ECLI results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    advice_indices : list\n",
    "        List of advice letter indices to compare\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"üîç Diagnosing Search Issue\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get advice letters\n",
    "    with engine.connect() as conn:\n",
    "        advice_docs = conn.execute(sqltext(\"\"\"\n",
    "            SELECT doc_id, text, raw_metadata, title\n",
    "            FROM documents \n",
    "            WHERE doc_type = 'advice'\n",
    "            ORDER BY doc_id\n",
    "        \"\"\")).mappings().all()\n",
    "    \n",
    "    if not advice_docs:\n",
    "        print(\"‚ö† No advice letters found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nComparing {len(advice_indices)} advice letters:\")\n",
    "    print(f\"Indices: {advice_indices}\")\n",
    "    \n",
    "    # Compare texts\n",
    "    texts = []\n",
    "    embeddings = []\n",
    "    \n",
    "    for idx in advice_indices:\n",
    "        if idx >= len(advice_docs):\n",
    "            print(f\"‚ö† Index {idx} out of range (max: {len(advice_docs)-1})\")\n",
    "            continue\n",
    "        \n",
    "        advice = advice_docs[idx]\n",
    "        text = advice['text']\n",
    "        texts.append(text)\n",
    "        \n",
    "        # Generate embedding\n",
    "        emb = model.encode([text], return_dense=True)[\"dense_vecs\"][0]\n",
    "        embeddings.append(emb)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Advice Letter {idx}:\")\n",
    "        print(f\"  Doc ID: {advice['doc_id']}\")\n",
    "        print(f\"  Text length: {len(text)} chars\")\n",
    "        print(f\"  First 200 chars: {text[:200]}...\")\n",
    "        print(f\"  Last 200 chars: ...{text[-200:]}\")\n",
    "    \n",
    "    # Compare embeddings\n",
    "    if len(embeddings) >= 2:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"Embedding Similarity Analysis:\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        for i in range(len(embeddings)):\n",
    "            for j in range(i+1, len(embeddings)):\n",
    "                # Cosine similarity\n",
    "                emb1 = np.array(embeddings[i])\n",
    "                emb2 = np.array(embeddings[j])\n",
    "                cosine_sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "                \n",
    "                # Euclidean distance\n",
    "                euclidean_dist = np.linalg.norm(emb1 - emb2)\n",
    "                \n",
    "                print(f\"\\nAdvice {advice_indices[i]} vs Advice {advice_indices[j]}:\")\n",
    "                print(f\"  Cosine Similarity: {cosine_sim:.4f} (1.0 = identical, 0.0 = orthogonal)\")\n",
    "                print(f\"  Euclidean Distance: {euclidean_dist:.4f}\")\n",
    "                \n",
    "                if cosine_sim > 0.95:\n",
    "                    print(f\"  ‚ö† WARNING: Embeddings are very similar! (>0.95)\")\n",
    "                    print(f\"     This explains why search results are the same.\")\n",
    "        \n",
    "        # Compare text similarity (simple word overlap)\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"Text Similarity (Word Overlap):\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        for i in range(len(texts)):\n",
    "            for j in range(i+1, len(texts)):\n",
    "                words1 = set(texts[i].lower().split())\n",
    "                words2 = set(texts[j].lower().split())\n",
    "                overlap = len(words1 & words2) / len(words1 | words2) if (words1 | words2) else 0\n",
    "                \n",
    "                print(f\"\\nAdvice {advice_indices[i]} vs Advice {advice_indices[j]}:\")\n",
    "                print(f\"  Word Overlap: {overlap:.4f} ({overlap*100:.1f}% shared words)\")\n",
    "                \n",
    "                if overlap > 0.8:\n",
    "                    print(f\"  ‚ö† WARNING: Texts are very similar! (>80% word overlap)\")\n",
    "    \n",
    "    # Test actual search results\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Actual Search Results Comparison:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for idx in advice_indices:\n",
    "        if idx >= len(advice_docs):\n",
    "            continue\n",
    "        \n",
    "        advice = advice_docs[idx]\n",
    "        text = advice['text']\n",
    "        results = find_relevant_ecli(text, top_n=5, min_score=0.2)\n",
    "        \n",
    "        print(f\"\\nAdvice {idx} results:\")\n",
    "        if results:\n",
    "            for i, r in enumerate(results[:3], 1):\n",
    "                print(f\"  {i}. {r['ecli_number']} (score: {r['score']:.4f})\")\n",
    "        else:\n",
    "            print(\"  No results\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üí° Diagnosis Complete\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "def test_with_existing_advice_letter(advice_index=None, top_n=10, min_score=0.3):\n",
    "    \"\"\"\n",
    "    Test RAG system using an existing advice letter from the database.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    advice_index : int, optional\n",
    "        Index of the advice letter to test (0-based). If None, randomly selects one.\n",
    "    top_n : int\n",
    "        Number of ECLI numbers to retrieve\n",
    "    min_score : float\n",
    "        Minimum relevance score threshold\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Test results with advice letter info, predicted ECLI, and ground truth (if available)\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    # Get advice letters from database\n",
    "    with engine.connect() as conn:\n",
    "        advice_docs = conn.execute(sqltext(\"\"\"\n",
    "            SELECT doc_id, text, raw_metadata, title\n",
    "            FROM documents \n",
    "            WHERE doc_type = 'advice'\n",
    "            ORDER BY doc_id\n",
    "        \"\"\")).mappings().all()\n",
    "    \n",
    "    if not advice_docs:\n",
    "        print(\"‚ö† No advice letters found in database. Please run import_all_data() first.\")\n",
    "        return None\n",
    "    \n",
    "    # Select advice letter\n",
    "    if advice_index is None:\n",
    "        advice_index = random.randint(0, len(advice_docs) - 1)\n",
    "    \n",
    "    if advice_index >= len(advice_docs):\n",
    "        print(f\"‚ö† Invalid index. Available: 0-{len(advice_docs)-1}\")\n",
    "        return None\n",
    "    \n",
    "    selected_advice = advice_docs[advice_index]\n",
    "    advice_text = selected_advice['text']\n",
    "    doc_id = selected_advice['doc_id']\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"Testing RAG System with Existing Advice Letter\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nSelected Advice Letter:\")\n",
    "    print(f\"  Index: {advice_index} / {len(advice_docs)-1}\")\n",
    "    print(f\"  Doc ID: {doc_id}\")\n",
    "    if selected_advice.get('title'):\n",
    "        print(f\"  Title: {selected_advice['title'][:100]}...\")\n",
    "    print(f\"  Text length: {len(advice_text)} characters\")\n",
    "    print(f\"\\n  Preview: {advice_text[:200]}...\")\n",
    "    \n",
    "    # Find relevant ECLI\n",
    "    # Option 1: Use issues-based retrieval (new strategy - recommended)\n",
    "    # Option 2: Use full document retrieval (original strategy)\n",
    "    use_issues_strategy = True  # Set to False to use original strategy\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Searching for relevant ECLI numbers...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if use_issues_strategy:\n",
    "        print(\"Using issues-based retrieval strategy:\")\n",
    "        print(\"  - Split advice letter into 5 issues\")\n",
    "        print(\"  - Each issue: Retrieve@200, Rerank@50\")\n",
    "        print(\"  - Aggregate by ECLI and return top 5 with best evidence chunks\")\n",
    "        results = find_relevant_ecli_by_issues(\n",
    "            advice_text,\n",
    "            top_n=top_n,\n",
    "            num_issues=5,\n",
    "            retrieve_k=200,\n",
    "            rerank_k=50,\n",
    "            min_score=min_score\n",
    "        )\n",
    "    else:\n",
    "        print(\"Using full document retrieval strategy:\")\n",
    "        # Ensure include_popular is enabled to include highly-cited ECLI\n",
    "        results = find_relevant_ecli(\n",
    "            advice_text, \n",
    "            top_n=top_n, \n",
    "            min_score=min_score,\n",
    "            include_popular=True,\n",
    "            popular_min_citations=100\n",
    "        )\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n‚úì Found {len(results)} relevant ECLI numbers:\")\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"  {i}. {r['ecli_number']} (score: {r['score']:.4f})\")\n",
    "        if 'issue_idx' in r:\n",
    "            print(f\"     From Issue: {r['issue_idx'] + 1}/5\")\n",
    "        if 'best_evidence_chunk' in r:\n",
    "            print(f\"     Best Evidence: {r['best_evidence_chunk'][:200]}...\")\n",
    "        else:\n",
    "            print(f\"     Snippet: {r['text_snippet'][:150]}...\")\n",
    "    \n",
    "    # Try to get ground truth\n",
    "    ground_truth_ecli = None\n",
    "    try:\n",
    "        gt = load_ground_truth_ecli()\n",
    "        # Try to match by doc_id or metadata\n",
    "        if doc_id in gt:\n",
    "            ground_truth_ecli = gt[doc_id]\n",
    "        else:\n",
    "            # Try metadata\n",
    "            metadata = selected_advice.get('raw_metadata')\n",
    "            if metadata:\n",
    "                if isinstance(metadata, dict):\n",
    "                    zaaknummer = metadata.get('Octopus zaaknummer') or metadata.get('zaaknummer')\n",
    "                    if zaaknummer and str(zaaknummer) in gt:\n",
    "                        ground_truth_ecli = gt[str(zaaknummer)]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Compare with ground truth if available\n",
    "    if ground_truth_ecli:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"Ground Truth Comparison\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Ensure ground_truth_ecli is a list\n",
    "        if isinstance(ground_truth_ecli, str):\n",
    "            ground_truth_ecli = [ground_truth_ecli]\n",
    "        elif not isinstance(ground_truth_ecli, list):\n",
    "            ground_truth_ecli = [str(ground_truth_ecli)]\n",
    "        \n",
    "        print(f\"Expected ECLI ({len(ground_truth_ecli)} total):\")\n",
    "        for i, ecli in enumerate(ground_truth_ecli, 1):\n",
    "            print(f\"  {i}. {ecli}\")\n",
    "        \n",
    "        predicted_ecli = [r['ecli_number'] for r in results]\n",
    "        found_correct = [ecli for ecli in predicted_ecli if ecli in ground_truth_ecli]\n",
    "        \n",
    "        if found_correct:\n",
    "            print(f\"\\n‚úì Found {len(found_correct)}/{len(ground_truth_ecli)} correct ECLI in results:\")\n",
    "            for ecli in found_correct:\n",
    "                rank = predicted_ecli.index(ecli) + 1\n",
    "                print(f\"  - {ecli} (rank: {rank})\")\n",
    "            \n",
    "            # Show missing ECLI if any\n",
    "            missing = [ecli for ecli in ground_truth_ecli if ecli not in found_correct]\n",
    "            if missing:\n",
    "                print(f\"\\n‚úó Missing {len(missing)} ECLI from ground truth:\")\n",
    "                for ecli in missing:\n",
    "                    print(f\"  - {ecli}\")\n",
    "        else:\n",
    "            print(f\"\\n‚úó No correct ECLI found in results (expected {len(ground_truth_ecli)} ECLI)\")\n",
    "    \n",
    "    return {\n",
    "        'advice_index': advice_index,\n",
    "        'doc_id': doc_id,\n",
    "        'advice_text': advice_text[:500],  # First 500 chars\n",
    "        'predicted_ecli': [r['ecli_number'] for r in results],\n",
    "        'ground_truth_ecli': ground_truth_ecli,\n",
    "        'found_correct': found_correct if ground_truth_ecli else None\n",
    "    }\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ECLI Citation Finder - Ready to Use\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nOption 1: Test with existing advice letter from database\")\n",
    "print(\"  test_with_existing_advice_letter()  # Random selection\")\n",
    "print(\"  test_with_existing_advice_letter(advice_index=0)  # Specific index\")\n",
    "print(\"\\nOption 2: Use Word document\")\n",
    "print(\"  docx_path = 'your_advice_letter.docx'\")\n",
    "print(\"  advice_text = read_word_document(docx_path)\")\n",
    "print(\"  results = find_relevant_ecli(advice_text, top_n=10)\")\n",
    "print(\"  print(format_ecli_citations(results))\")\n",
    "print(\"\\nOption 3: Use text string\")\n",
    "print(\"  results = find_relevant_ecli('your advice text', top_n=10)\")\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b334e315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Evaluation functions ready!\n",
      "\n",
      "======================================================================\n",
      "How to Test and Calculate Accuracy:\n",
      "======================================================================\n",
      "\n",
      "1. üéØ Test ALL Advice Letters (Recommended for Full Accuracy):\n",
      "   metrics = evaluate_all_advice_letters(top_k=10, min_score=0.2)\n",
      "   print(f'Overall Accuracy: {metrics[\"accuracy\"]*100:.2f}%')\n",
      "   # This tests EVERY advice letter with ground truth\n",
      "   # Returns: Hit Rate@K (Accuracy), Precision, Recall, F1, MRR\n",
      "\n",
      "2. Quick Test (Sample of 50):\n",
      "   metrics = evaluate_with_ground_truth(test_size=50, top_k=10)\n",
      "   print(f'Accuracy: {metrics[\"hit_rate_at_k\"]*100:.2f}%')\n",
      "\n",
      "3. Single Query Test:\n",
      "   result = evaluate_single_query('advice text', expected_ecli='ECLI:NL:...')\n",
      "\n",
      "======================================================================\n",
      "üìä Key Metrics Explained:\n",
      "======================================================================\n",
      "\n",
      "  üéØ Hit Rate@K (Accuracy):\n",
      "     ‚Üí Percentage of queries with at least one correct ECLI in top K\n",
      "     ‚Üí This is the 'accuracy' you care about\n",
      "\n",
      "  üìä Precision@K:\n",
      "     ‚Üí Of retrieved ECLI, how many are correct\n",
      "\n",
      "  üìà Recall@K:\n",
      "     ‚Üí Of expected ECLI, how many were found\n",
      "\n",
      "  ‚öñÔ∏è  F1 Score:\n",
      "     ‚Üí Harmonic mean of Precision and Recall\n",
      "\n",
      "  üìç MRR:\n",
      "     ‚Üí Average position of first correct result (higher is better)\n",
      "\n",
      "======================================================================\n",
      "Example - Test All Advice Letters:\n",
      "======================================================================\n",
      "  # Test all advice letters and get overall accuracy\n",
      "  metrics = evaluate_all_advice_letters(top_k=10, min_score=0.2)\n",
      "  \n",
      "  print(f'\\nüéØ Overall Accuracy: {metrics[\"accuracy\"]*100:.2f}%')\n",
      "  print(f'üìä Precision: {metrics[\"precision\"]*100:.2f}%')\n",
      "  print(f'üìà Recall: {metrics[\"recall\"]*100:.2f}%')\n",
      "  print(f'‚öñÔ∏è  F1 Score: {metrics[\"f1\"]:.4f}')\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# RAG System Evaluation with Accuracy Metrics\n",
    "# ============================================\n",
    "# This cell provides comprehensive evaluation metrics including accuracy, precision, recall, and MRR\n",
    "# Uses ground truth ECLI numbers from the advice letters dataset\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_ground_truth_ecli():\n",
    "    \"\"\"\n",
    "    Load ground truth ECLI numbers from the advice letters Excel file.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary mapping advice letter IDs to list of expected ECLI numbers\n",
    "    \"\"\"\n",
    "    advice_file = Path.cwd() / \"Dataset Advice letters on objections towing of bicycles.xlsx\"\n",
    "    if not advice_file.exists():\n",
    "        print(f\"‚ö† Advice file not found: {advice_file}\")\n",
    "        return {}\n",
    "    \n",
    "    df = pd.read_excel(advice_file)\n",
    "    ground_truth = {}\n",
    "    \n",
    "    # Get ID column\n",
    "    id_col = None\n",
    "    for col in [\"Octopus zaaknummer\", \"zaaknummer\", \"id\", \"doc_id\"]:\n",
    "        if col in df.columns:\n",
    "            id_col = col\n",
    "            break\n",
    "    \n",
    "    if not id_col:\n",
    "        print(\"‚ö† Could not find ID column in advice file\")\n",
    "        return {}\n",
    "    \n",
    "    # Get ECLI column\n",
    "    if 'ECLI' not in df.columns:\n",
    "        print(\"‚ö† No ECLI column found in advice file\")\n",
    "        return {}\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        advice_id = str(row[id_col])\n",
    "        ecli_value = row['ECLI']\n",
    "        \n",
    "        # Parse ECLI (might be string representation of list or actual list)\n",
    "        ecli_list = []\n",
    "        if pd.notna(ecli_value):\n",
    "            if isinstance(ecli_value, str):\n",
    "                try:\n",
    "                    # Try to parse as Python list\n",
    "                    ecli_list = ast.literal_eval(ecli_value)\n",
    "                except:\n",
    "                    # If parsing fails, treat as single ECLI\n",
    "                    ecli_list = [ecli_value]\n",
    "            elif isinstance(ecli_value, list):\n",
    "                ecli_list = ecli_value\n",
    "            else:\n",
    "                ecli_list = [str(ecli_value)]\n",
    "        \n",
    "        # Normalize ECLI numbers (remove duplicates, ensure they're strings)\n",
    "        ecli_list = list(set([str(e).strip() for e in ecli_list if pd.notna(e) and str(e).strip()]))\n",
    "        if ecli_list:\n",
    "            ground_truth[advice_id] = ecli_list\n",
    "    \n",
    "    # Only print once using a function attribute to track if already printed\n",
    "    if not hasattr(load_ground_truth_ecli, '_printed'):\n",
    "        print(f\"‚úì Loaded ground truth for {len(ground_truth)} advice letters\")\n",
    "        load_ground_truth_ecli._printed = True\n",
    "    return ground_truth\n",
    "\n",
    "def analyze_why_popular_ecli_not_selected(test_size=100):\n",
    "    \"\"\"\n",
    "    Analyze why popular ECLI (that are frequently cited in ground truth) \n",
    "    are not being selected by the RAG system.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    test_size : int\n",
    "        Number of advice letters to analyze\n",
    "    \"\"\"\n",
    "    # Load ground truth\n",
    "    ground_truth = load_ground_truth_ecli()\n",
    "    if not ground_truth:\n",
    "        print(\"‚ö† Cannot analyze: No ground truth data available\")\n",
    "        return None\n",
    "    \n",
    "    # Count ECLI frequency in ground truth\n",
    "    ecli_frequency = {}\n",
    "    for zaaknummer, ecli_list in ground_truth.items():\n",
    "        for ecli in ecli_list:\n",
    "            ecli_frequency[ecli] = ecli_frequency.get(ecli, 0) + 1\n",
    "    \n",
    "    # Find most popular ECLI\n",
    "    sorted_ecli = sorted(ecli_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_ecli = sorted_ecli[:10]  # Top 10 most cited ECLI\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Analysis: Why Popular ECLI Are Not Selected\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nTop 10 Most Cited ECLI in Ground Truth:\")\n",
    "    for i, (ecli, count) in enumerate(top_ecli, 1):\n",
    "        percentage = (count / len(ground_truth)) * 100\n",
    "        print(f\"  {i}. {ecli}: cited in {count} advice letters ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Get sample advice letters\n",
    "    with engine.connect() as conn:\n",
    "        advice_docs = conn.execute(sqltext(\"\"\"\n",
    "            SELECT doc_id, text, raw_metadata \n",
    "            FROM documents \n",
    "            WHERE doc_type = 'advice'\n",
    "            LIMIT :limit\n",
    "        \"\"\"), {\"limit\": test_size}).mappings().all()\n",
    "    \n",
    "    if not advice_docs:\n",
    "        print(\"‚ö† No advice documents found\")\n",
    "        return None\n",
    "    \n",
    "    # Analyze how often popular ECLI are selected\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Selection Analysis for Popular ECLI\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    popular_ecli_set = set([ecli for ecli, _ in top_ecli])\n",
    "    \n",
    "    for ecli, citation_count in top_ecli[:5]:  # Analyze top 5\n",
    "        selected_count = 0\n",
    "        should_be_selected_count = 0\n",
    "        avg_rank_when_selected = []\n",
    "        avg_score_when_selected = []\n",
    "        \n",
    "        for doc in advice_docs:\n",
    "            doc_id = doc['doc_id']\n",
    "            advice_text = doc['text']\n",
    "            \n",
    "            # Check if this ECLI should be selected (in ground truth)\n",
    "            expected_ecli = None\n",
    "            if doc_id in ground_truth:\n",
    "                expected_ecli = ground_truth[doc_id]\n",
    "            else:\n",
    "                metadata = doc.get('raw_metadata')\n",
    "                if metadata and isinstance(metadata, dict):\n",
    "                    zaaknummer = metadata.get('Octopus zaaknummer') or metadata.get('zaaknummer')\n",
    "                    if zaaknummer and str(zaaknummer) in ground_truth:\n",
    "                        expected_ecli = ground_truth[str(zaaknummer)]\n",
    "            \n",
    "            if not expected_ecli:\n",
    "                continue\n",
    "            \n",
    "            if ecli in expected_ecli:\n",
    "                should_be_selected_count += 1\n",
    "                \n",
    "                # Check if it was actually selected\n",
    "                predicted_ecli = find_relevant_ecli(advice_text, top_n=10, min_score=0.2)\n",
    "                predicted_ecli_numbers = [r['ecli_number'] for r in predicted_ecli]\n",
    "                predicted_scores = [r['score'] for r in predicted_ecli]\n",
    "                \n",
    "                if ecli in predicted_ecli_numbers:\n",
    "                    selected_count += 1\n",
    "                    rank = predicted_ecli_numbers.index(ecli) + 1\n",
    "                    score = predicted_scores[predicted_ecli_numbers.index(ecli)]\n",
    "                    avg_rank_when_selected.append(rank)\n",
    "                    avg_score_when_selected.append(score)\n",
    "        \n",
    "        if should_be_selected_count > 0:\n",
    "            recall = selected_count / should_be_selected_count\n",
    "            avg_rank = sum(avg_rank_when_selected) / len(avg_rank_when_selected) if avg_rank_when_selected else 0\n",
    "            avg_score = sum(avg_score_when_selected) / len(avg_score_when_selected) if avg_score_when_selected else 0\n",
    "            \n",
    "            print(f\"ECLI: {ecli}\")\n",
    "            print(f\"  Should be selected: {should_be_selected_count} times\")\n",
    "            print(f\"  Actually selected: {selected_count} times\")\n",
    "            print(f\"  Recall: {recall:.2%}\")\n",
    "            if avg_rank_when_selected:\n",
    "                print(f\"  Avg rank when selected: {avg_rank:.1f}\")\n",
    "                print(f\"  Avg score when selected: {avg_score:.4f}\")\n",
    "            print()\n",
    "    \n",
    "    return {\n",
    "        'top_ecli': top_ecli,\n",
    "        'popular_ecli_set': popular_ecli_set\n",
    "    }\n",
    "\n",
    "def analyze_performance_issues(test_size=100):\n",
    "    \"\"\"\n",
    "    Analyze why RAG performance is poor and provide insights.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    test_size : int\n",
    "        Number of advice letters to analyze\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Load ground truth\n",
    "    ground_truth = load_ground_truth_ecli()\n",
    "    if not ground_truth:\n",
    "        print(\"‚ö† Cannot analyze: No ground truth data available\")\n",
    "        return None\n",
    "    \n",
    "    # Get sample advice letters\n",
    "    with engine.connect() as conn:\n",
    "        advice_docs = conn.execute(sqltext(\"\"\"\n",
    "            SELECT doc_id, text, raw_metadata \n",
    "            FROM documents \n",
    "            WHERE doc_type = 'advice'\n",
    "            LIMIT :limit\n",
    "        \"\"\"), {\"limit\": test_size}).mappings().all()\n",
    "    \n",
    "    if not advice_docs:\n",
    "        print(\"‚ö† No advice documents found\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Performance Analysis\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Analyzing {len(advice_docs)} advice letters\\n\")\n",
    "    \n",
    "    # Collect statistics\n",
    "    score_distributions = []\n",
    "    precision_by_score_range = {}\n",
    "    recall_by_score_range = {}\n",
    "    \n",
    "    queries_with_low_scores = 0\n",
    "    queries_with_high_scores = 0\n",
    "    queries_with_correct = 0\n",
    "    queries_with_wrong = 0\n",
    "    \n",
    "    for doc in advice_docs:\n",
    "        doc_id = doc['doc_id']\n",
    "        advice_text = doc['text']\n",
    "        \n",
    "        # Get ground truth\n",
    "        expected_ecli = None\n",
    "        if doc_id in ground_truth:\n",
    "            expected_ecli = ground_truth[doc_id]\n",
    "        else:\n",
    "            metadata = doc.get('raw_metadata')\n",
    "            if metadata and isinstance(metadata, dict):\n",
    "                zaaknummer = metadata.get('Octopus zaaknummer') or metadata.get('zaaknummer')\n",
    "                if zaaknummer and str(zaaknummer) in ground_truth:\n",
    "                    expected_ecli = ground_truth[str(zaaknummer)]\n",
    "        \n",
    "        if not expected_ecli:\n",
    "            continue\n",
    "        \n",
    "        # Get RAG results\n",
    "        predicted_ecli = find_relevant_ecli(advice_text, top_n=10, min_score=0.2)\n",
    "        predicted_ecli_numbers = [r['ecli_number'] for r in predicted_ecli]\n",
    "        predicted_scores = [r['score'] for r in predicted_ecli]\n",
    "        \n",
    "        if predicted_scores:\n",
    "            score_distributions.extend(predicted_scores)\n",
    "            avg_score = np.mean(predicted_scores)\n",
    "            \n",
    "            # Categorize by score range\n",
    "            if avg_score < 0.3:\n",
    "                score_range = \"low (<0.3)\"\n",
    "            elif avg_score < 0.5:\n",
    "                score_range = \"medium (0.3-0.5)\"\n",
    "            else:\n",
    "                score_range = \"high (>0.5)\"\n",
    "            \n",
    "            if score_range not in precision_by_score_range:\n",
    "                precision_by_score_range[score_range] = {'precision': [], 'recall': [], 'count': 0}\n",
    "            \n",
    "            # Calculate metrics\n",
    "            expected_set = set(expected_ecli)\n",
    "            predicted_set = set(predicted_ecli_numbers)\n",
    "            \n",
    "            if len(predicted_set) > 0:\n",
    "                precision = len(expected_set & predicted_set) / len(predicted_set)\n",
    "                precision_by_score_range[score_range]['precision'].append(precision)\n",
    "            \n",
    "            if len(expected_set) > 0:\n",
    "                recall = len(expected_set & predicted_set) / len(expected_set)\n",
    "                precision_by_score_range[score_range]['recall'].append(recall)\n",
    "            \n",
    "            precision_by_score_range[score_range]['count'] += 1\n",
    "            \n",
    "            # Check if correct ECLI found\n",
    "            if expected_set & predicted_set:\n",
    "                queries_with_correct += 1\n",
    "                if avg_score >= 0.4:\n",
    "                    queries_with_high_scores += 1\n",
    "            else:\n",
    "                queries_with_wrong += 1\n",
    "                if avg_score < 0.3:\n",
    "                    queries_with_low_scores += 1\n",
    "    \n",
    "    # Print analysis\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Score Distribution Analysis\")\n",
    "    print(f\"{'='*70}\")\n",
    "    if score_distributions:\n",
    "        print(f\"  Average score: {np.mean(score_distributions):.4f}\")\n",
    "        print(f\"  Median score: {np.median(score_distributions):.4f}\")\n",
    "        print(f\"  Min score: {np.min(score_distributions):.4f}\")\n",
    "        print(f\"  Max score: {np.max(score_distributions):.4f}\")\n",
    "        print(f\"  Std deviation: {np.std(score_distributions):.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Performance by Score Range\")\n",
    "    print(f\"{'='*70}\")\n",
    "    for score_range in sorted(precision_by_score_range.keys()):\n",
    "        data = precision_by_score_range[score_range]\n",
    "        if data['count'] > 0:\n",
    "            avg_precision = np.mean(data['precision']) if data['precision'] else 0.0\n",
    "            avg_recall = np.mean(data['recall']) if data['recall'] else 0.0\n",
    "            print(f\"\\n{score_range}:\")\n",
    "            print(f\"  Queries: {data['count']}\")\n",
    "            print(f\"  Avg Precision: {avg_precision:.4f} ({avg_precision*100:.2f}%)\")\n",
    "            print(f\"  Avg Recall: {avg_recall:.4f} ({avg_recall*100:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Key Insights\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Queries with correct ECLI found: {queries_with_correct}\")\n",
    "    print(f\"  Queries with wrong ECLI only: {queries_with_wrong}\")\n",
    "    print(f\"  Queries with high scores (>0.4) and correct: {queries_with_high_scores}\")\n",
    "    print(f\"  Queries with low scores (<0.3) and wrong: {queries_with_low_scores}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üí° Recommendations\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if score_distributions:\n",
    "        if np.mean(score_distributions) < 0.3:\n",
    "            print(\"  1. ‚ö† Average scores are low - consider:\")\n",
    "            print(\"     - Lowering min_score threshold\")\n",
    "            print(\"     - Improving embedding quality\")\n",
    "            print(\"     - Checking if ECLI data is relevant\")\n",
    "        \n",
    "        if np.std(score_distributions) < 0.1:\n",
    "            print(\"  2. ‚ö† Score distribution is narrow - scores are too similar\")\n",
    "            print(\"     - This suggests normalization may be flattening differences\")\n",
    "            print(\"     - Consider using raw dense scores for ranking\")\n",
    "        \n",
    "        if queries_with_wrong > queries_with_correct:\n",
    "            print(\"  3. ‚ö† More wrong results than correct - precision is low\")\n",
    "            print(\"     - Increase min_score threshold\")\n",
    "            print(\"     - Use stricter filtering\")\n",
    "            print(\"     - Consider reranking with better models\")\n",
    "    \n",
    "    return {\n",
    "        'score_stats': {\n",
    "            'mean': np.mean(score_distributions) if score_distributions else 0,\n",
    "            'median': np.median(score_distributions) if score_distributions else 0,\n",
    "            'std': np.std(score_distributions) if score_distributions else 0\n",
    "        },\n",
    "        'precision_by_range': precision_by_score_range,\n",
    "        'queries_correct': queries_with_correct,\n",
    "        'queries_wrong': queries_with_wrong\n",
    "    }\n",
    "\n",
    "def find_optimal_threshold(top_k=10, test_size=100):\n",
    "    \"\"\"\n",
    "    Find optimal min_score threshold by testing different values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    top_k : int\n",
    "        Number of ECLI results to retrieve per query\n",
    "    test_size : int\n",
    "        Number of advice letters to test (for speed)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Results for different threshold values\n",
    "    \"\"\"\n",
    "    # Load ground truth\n",
    "    ground_truth = load_ground_truth_ecli()\n",
    "    if not ground_truth:\n",
    "        print(\"‚ö† Cannot evaluate: No ground truth data available\")\n",
    "        return None\n",
    "    \n",
    "    # Get sample advice letters\n",
    "    with engine.connect() as conn:\n",
    "        advice_docs = conn.execute(sqltext(\"\"\"\n",
    "            SELECT doc_id, text, raw_metadata \n",
    "            FROM documents \n",
    "            WHERE doc_type = 'advice'\n",
    "            LIMIT :limit\n",
    "        \"\"\"), {\"limit\": test_size}).mappings().all()\n",
    "    \n",
    "    if not advice_docs:\n",
    "        print(\"‚ö† No advice documents found\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Finding Optimal Threshold\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Testing {len(advice_docs)} advice letters\")\n",
    "    print(f\"Top-K: {top_k}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Test different thresholds\n",
    "    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "    results = {}\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        print(f\"Testing min_score = {threshold}...\")\n",
    "        \n",
    "        hits = 0\n",
    "        total_precision = 0.0\n",
    "        total_recall = 0.0\n",
    "        queries_tested = 0\n",
    "        avg_results_per_query = 0.0\n",
    "        \n",
    "        for doc in advice_docs:\n",
    "            doc_id = doc['doc_id']\n",
    "            advice_text = doc['text']\n",
    "            \n",
    "            # Get ground truth\n",
    "            expected_ecli = None\n",
    "            if doc_id in ground_truth:\n",
    "                expected_ecli = ground_truth[doc_id]\n",
    "            else:\n",
    "                metadata = doc.get('raw_metadata')\n",
    "                if metadata and isinstance(metadata, dict):\n",
    "                    zaaknummer = metadata.get('Octopus zaaknummer') or metadata.get('zaaknummer')\n",
    "                    if zaaknummer and str(zaaknummer) in ground_truth:\n",
    "                        expected_ecli = ground_truth[str(zaaknummer)]\n",
    "            \n",
    "            if not expected_ecli:\n",
    "                continue\n",
    "            \n",
    "            queries_tested += 1\n",
    "            \n",
    "            # Get RAG results with reranker and popular ECLI enabled\n",
    "            # Disable keyword_filter (it filters out 61.3% of ground truth ECLI)\n",
    "            # Enable include_popular to include highly-cited ECLI\n",
    "            predicted_ecli = find_relevant_ecli(\n",
    "                advice_text, \n",
    "                top_n=top_k, \n",
    "                min_score=threshold,\n",
    "                keyword_filter=False,  # Disable keyword filtering (too restrictive)\n",
    "                use_reranker=True,     # Enable reranker\n",
    "                rerank_top_k=50,      # Retrieve top 50 candidates for reranking\n",
    "                include_popular=True,  # Include highly-cited ECLI\n",
    "                popular_min_citations=100  # Include ECLI cited 100+ times\n",
    "            )\n",
    "            predicted_ecli_numbers = [r['ecli_number'] for r in predicted_ecli]\n",
    "            \n",
    "            avg_results_per_query += len(predicted_ecli_numbers)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            expected_set = set(expected_ecli)\n",
    "            predicted_set = set(predicted_ecli_numbers)\n",
    "            \n",
    "            # Hit at K\n",
    "            if expected_set & predicted_set:\n",
    "                hits += 1\n",
    "            \n",
    "            # Precision\n",
    "            if len(predicted_set) > 0:\n",
    "                precision = len(expected_set & predicted_set) / len(predicted_set)\n",
    "                total_precision += precision\n",
    "            \n",
    "            # Recall\n",
    "            if len(expected_set) > 0:\n",
    "                recall = len(expected_set & predicted_set) / len(expected_set)\n",
    "                total_recall += recall\n",
    "        \n",
    "        if queries_tested > 0:\n",
    "            hit_rate = hits / queries_tested\n",
    "            avg_precision = total_precision / queries_tested\n",
    "            avg_recall = total_recall / queries_tested\n",
    "            avg_results = avg_results_per_query / queries_tested\n",
    "            \n",
    "            results[threshold] = {\n",
    "                'hit_rate': hit_rate,\n",
    "                'precision': avg_precision,\n",
    "                'recall': avg_recall,\n",
    "                'avg_results': avg_results,\n",
    "                'queries_tested': queries_tested\n",
    "            }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Threshold Comparison Results\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\n{'Threshold':<12} {'Hit Rate':<12} {'Precision':<12} {'Recall':<12} {'Avg Results':<12}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    best_threshold = None\n",
    "    best_score = 0.0\n",
    "    \n",
    "    for threshold in sorted(results.keys()):\n",
    "        r = results[threshold]\n",
    "        print(f\"{threshold:<12.1f} {r['hit_rate']*100:<12.2f}% {r['precision']*100:<12.2f}% {r['recall']*100:<12.2f}% {r['avg_results']:<12.2f}\")\n",
    "        \n",
    "        # Score: balance between hit rate and precision\n",
    "        # Use F1-like score: 2 * (hit_rate * precision) / (hit_rate + precision)\n",
    "        if r['hit_rate'] + r['precision'] > 0:\n",
    "            score = 2 * (r['hit_rate'] * r['precision']) / (r['hit_rate'] + r['precision'])\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_threshold = threshold\n",
    "    \n",
    "    if best_threshold:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üí° Recommended Threshold: {best_threshold}\")\n",
    "        print(f\"   Hit Rate: {results[best_threshold]['hit_rate']*100:.2f}%\")\n",
    "        print(f\"   Precision: {results[best_threshold]['precision']*100:.2f}%\")\n",
    "        print(f\"   Recall: {results[best_threshold]['recall']*100:.2f}%\")\n",
    "        print(f\"{'='*70}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_all_advice_letters(top_k=10, min_score=0.2):\n",
    "    \"\"\"\n",
    "    Evaluate RAG system on ALL advice letters with ground truth.\n",
    "    This function tests every advice letter that has ground truth ECLI.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    top_k : int\n",
    "        Number of ECLI results to retrieve per query (default: 10)\n",
    "    min_score : float\n",
    "        Minimum relevance score threshold (default: 0.2)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Comprehensive evaluation metrics for all advice letters\n",
    "    \"\"\"\n",
    "    # Load ground truth\n",
    "    ground_truth = load_ground_truth_ecli()\n",
    "    if not ground_truth:\n",
    "        print(\"‚ö† Cannot evaluate: No ground truth data available\")\n",
    "        return None\n",
    "    \n",
    "    # Get ALL advice letters from database (no limit)\n",
    "    with engine.connect() as conn:\n",
    "        advice_docs = conn.execute(sqltext(\"\"\"\n",
    "            SELECT doc_id, text, raw_metadata \n",
    "            FROM documents \n",
    "            WHERE doc_type = 'advice'\n",
    "        \"\"\")).mappings().all()\n",
    "    \n",
    "    if not advice_docs:\n",
    "        print(\"‚ö† No advice documents found in database. Please run import_all_data() first.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Evaluating RAG System on ALL Advice Letters\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total advice letters in database: {len(advice_docs)}\")\n",
    "    print(f\"Ground truth available for: {len(ground_truth)} advice letters\")\n",
    "    print(f\"Top-K: {top_k}\")\n",
    "    print(f\"Minimum score: {min_score}\")\n",
    "    \n",
    "    # Check reranker status\n",
    "    reranker_enabled = USE_RERANKER and reranker is not None\n",
    "    if reranker_enabled:\n",
    "        print(f\"‚úì Reranker: ENABLED (will rerank top 50 candidates)\")\n",
    "    else:\n",
    "        print(f\"‚ö† Reranker: DISABLED (USE_RERANKER={USE_RERANKER}, reranker={'loaded' if reranker else 'not loaded'})\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    total_queries = 0\n",
    "    queries_with_ground_truth = 0\n",
    "    queries_with_results = 0\n",
    "    \n",
    "    # Precision and Recall metrics\n",
    "    total_precision = 0.0\n",
    "    total_recall = 0.0\n",
    "    total_f1 = 0.0\n",
    "    \n",
    "    # MRR (Mean Reciprocal Rank)\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    # Hit rate at K\n",
    "    hits_at_k = 0\n",
    "    \n",
    "    # Detailed results\n",
    "    detailed_results = []\n",
    "    \n",
    "    # Progress tracking\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    for doc in tqdm(advice_docs, desc=\"Evaluating advice letters\"):\n",
    "        doc_id = doc['doc_id']\n",
    "        advice_text = doc['text']\n",
    "        \n",
    "        # Check if we have ground truth for this advice letter\n",
    "        expected_ecli = None\n",
    "        \n",
    "        # Method 1: Try direct doc_id match\n",
    "        if doc_id in ground_truth:\n",
    "            expected_ecli = ground_truth[doc_id]\n",
    "        else:\n",
    "            # Method 2: Try to extract from raw_metadata\n",
    "            metadata = doc.get('raw_metadata')\n",
    "            if metadata:\n",
    "                if isinstance(metadata, dict):\n",
    "                    zaaknummer = metadata.get('Octopus zaaknummer') or metadata.get('zaaknummer')\n",
    "                    if zaaknummer and str(zaaknummer) in ground_truth:\n",
    "                        expected_ecli = ground_truth[str(zaaknummer)]\n",
    "                elif isinstance(metadata, str):\n",
    "                    try:\n",
    "                        import json\n",
    "                        meta_dict = json.loads(metadata)\n",
    "                        zaaknummer = meta_dict.get('Octopus zaaknummer') or meta_dict.get('zaaknummer')\n",
    "                        if zaaknummer and str(zaaknummer) in ground_truth:\n",
    "                            expected_ecli = ground_truth[str(zaaknummer)]\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        if not expected_ecli:\n",
    "            continue  # Skip if no ground truth\n",
    "        \n",
    "        queries_with_ground_truth += 1\n",
    "        total_queries += 1\n",
    "        \n",
    "        # Get RAG results\n",
    "        predicted_ecli = find_relevant_ecli(advice_text, top_n=top_k, min_score=min_score)\n",
    "        predicted_ecli_numbers = [r['ecli_number'] for r in predicted_ecli]\n",
    "        \n",
    "        if predicted_ecli_numbers:\n",
    "            queries_with_results += 1\n",
    "        \n",
    "        # Calculate metrics\n",
    "        expected_set = set(expected_ecli)\n",
    "        predicted_set = set(predicted_ecli_numbers)\n",
    "        \n",
    "        # Precision: relevant retrieved / total retrieved\n",
    "        if len(predicted_set) > 0:\n",
    "            precision = len(expected_set & predicted_set) / len(predicted_set)\n",
    "        else:\n",
    "            precision = 0.0\n",
    "        \n",
    "        # Recall: relevant retrieved / total relevant\n",
    "        if len(expected_set) > 0:\n",
    "            recall = len(expected_set & predicted_set) / len(expected_set)\n",
    "        else:\n",
    "            recall = 0.0\n",
    "        \n",
    "        # F1 score\n",
    "        if precision + recall > 0:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            f1 = 0.0\n",
    "        \n",
    "        # MRR: Find rank of first relevant result\n",
    "        reciprocal_rank = 0.0\n",
    "        for rank, ecli in enumerate(predicted_ecli_numbers, 1):\n",
    "            if ecli in expected_set:\n",
    "                reciprocal_rank = 1.0 / rank\n",
    "                break\n",
    "        \n",
    "        # Hit at K: At least one relevant result in top K\n",
    "        hit_at_k = 1.0 if (expected_set & predicted_set) else 0.0\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "        reciprocal_ranks.append(reciprocal_rank)\n",
    "        hits_at_k += hit_at_k\n",
    "        \n",
    "        detailed_results.append({\n",
    "            'doc_id': doc_id,\n",
    "            'expected_ecli': expected_ecli,\n",
    "            'predicted_ecli': predicted_ecli_numbers,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'reciprocal_rank': reciprocal_rank,\n",
    "            'hit_at_k': hit_at_k\n",
    "        })\n",
    "    \n",
    "    # Calculate averages\n",
    "    if total_queries > 0:\n",
    "        avg_precision = total_precision / total_queries\n",
    "        avg_recall = total_recall / total_queries\n",
    "        avg_f1 = total_f1 / total_queries\n",
    "        mrr = sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "        hit_rate_at_k = hits_at_k / total_queries\n",
    "    else:\n",
    "        avg_precision = avg_recall = avg_f1 = mrr = hit_rate_at_k = 0.0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Evaluation Results - ALL Advice Letters\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Total advice letters in database: {len(advice_docs)}\")\n",
    "    print(f\"  Advice letters with ground truth: {queries_with_ground_truth}\")\n",
    "    print(f\"  Advice letters evaluated: {total_queries}\")\n",
    "    print(f\"  Queries with results: {queries_with_results} ({queries_with_results/total_queries*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Accuracy Metrics (Top-{top_k}):\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  üéØ Hit Rate@{top_k} (Accuracy): {hit_rate_at_k:.4f} ({hit_rate_at_k*100:.2f}%)\")\n",
    "    print(f\"     ‚Üí Percentage of queries with at least one correct ECLI in top {top_k}\")\n",
    "    print(f\"\\n  üìä Precision@{top_k}: {avg_precision:.4f} ({avg_precision*100:.2f}%)\")\n",
    "    print(f\"     ‚Üí Of retrieved ECLI, how many are correct\")\n",
    "    print(f\"\\n  üìà Recall@{top_k}: {avg_recall:.4f} ({avg_recall*100:.2f}%)\")\n",
    "    print(f\"     ‚Üí Of expected ECLI, how many were found\")\n",
    "    print(f\"\\n  ‚öñÔ∏è  F1 Score@{top_k}: {avg_f1:.4f}\")\n",
    "    print(f\"     ‚Üí Harmonic mean of Precision and Recall\")\n",
    "    print(f\"\\n  üìç MRR (Mean Reciprocal Rank): {mrr:.4f}\")\n",
    "    print(f\"     ‚Üí Average position of first correct result (higher is better)\")\n",
    "    \n",
    "    # Additional statistics\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Additional Statistics:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Count queries by number of expected ECLI\n",
    "    single_ecli_count = sum(1 for r in detailed_results if len(r['expected_ecli']) == 1)\n",
    "    multi_ecli_count = sum(1 for r in detailed_results if len(r['expected_ecli']) > 1)\n",
    "    \n",
    "    print(f\"  Queries with single ECLI in ground truth: {single_ecli_count}\")\n",
    "    print(f\"  Queries with multiple ECLI in ground truth: {multi_ecli_count}\")\n",
    "    \n",
    "    # Average number of expected ECLI per query\n",
    "    avg_expected = sum(len(r['expected_ecli']) for r in detailed_results) / len(detailed_results) if detailed_results else 0\n",
    "    print(f\"  Average expected ECLI per query: {avg_expected:.2f}\")\n",
    "    \n",
    "    # Average number of predicted ECLI per query\n",
    "    avg_predicted = sum(len(r['predicted_ecli']) for r in detailed_results) / len(detailed_results) if detailed_results else 0\n",
    "    print(f\"  Average predicted ECLI per query: {avg_predicted:.2f}\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Sample Results (First 5 queries):\")\n",
    "    print(f\"{'='*70}\")\n",
    "    for i, result in enumerate(detailed_results[:5], 1):\n",
    "        print(f\"\\nQuery {i}: {result['doc_id']}\")\n",
    "        print(f\"  Expected: {len(result['expected_ecli'])} ECLI\")\n",
    "        print(f\"  Predicted: {len(result['predicted_ecli'])} ECLI\")\n",
    "        print(f\"  Hit@K: {'‚úì' if result['hit_at_k'] > 0 else '‚úó'}\")\n",
    "        print(f\"  Precision: {result['precision']:.2f}, Recall: {result['recall']:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'total_queries': total_queries,\n",
    "        'queries_with_results': queries_with_results,\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1': avg_f1,\n",
    "        'mrr': mrr,\n",
    "        'hit_rate_at_k': hit_rate_at_k,\n",
    "        'accuracy': hit_rate_at_k,  # Alias for clarity\n",
    "        'detailed_results': detailed_results\n",
    "    }\n",
    "\n",
    "def evaluate_with_ground_truth(test_size=50, top_k=10, min_score=0.2):\n",
    "    \"\"\"\n",
    "    Evaluate RAG system using ground truth ECLI numbers from advice letters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    test_size : int\n",
    "        Number of advice letters to test\n",
    "    top_k : int\n",
    "        Number of ECLI results to retrieve per query\n",
    "    min_score : float\n",
    "        Minimum relevance score threshold\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Comprehensive evaluation metrics\n",
    "    \"\"\"\n",
    "    # Load ground truth\n",
    "    ground_truth = load_ground_truth_ecli()\n",
    "    if not ground_truth:\n",
    "        print(\"‚ö† Cannot evaluate: No ground truth data available\")\n",
    "        return None\n",
    "    \n",
    "    # Get advice letters from database\n",
    "    with engine.connect() as conn:\n",
    "        advice_docs = conn.execute(sqltext(\"\"\"\n",
    "            SELECT doc_id, text, raw_metadata \n",
    "            FROM documents \n",
    "            WHERE doc_type = 'advice'\n",
    "            LIMIT :limit\n",
    "        \"\"\"), {\"limit\": test_size}).mappings().all()\n",
    "    \n",
    "    if not advice_docs:\n",
    "        print(\"‚ö† No advice documents found in database. Please run import_all_data() first.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Evaluating RAG System with Ground Truth\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Test size: {test_size}\")\n",
    "    print(f\"Top-K: {top_k}\")\n",
    "    print(f\"Minimum score: {min_score}\")\n",
    "    print(f\"Ground truth coverage: {len(ground_truth)}/{len(advice_docs)} advice letters\")\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    total_queries = 0\n",
    "    queries_with_ground_truth = 0\n",
    "    queries_with_results = 0\n",
    "    \n",
    "    # Precision and Recall metrics\n",
    "    total_precision = 0.0\n",
    "    total_recall = 0.0\n",
    "    total_f1 = 0.0\n",
    "    \n",
    "    # MRR (Mean Reciprocal Rank)\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    # Hit rate at K\n",
    "    hits_at_k = 0\n",
    "    \n",
    "    # Detailed results\n",
    "    detailed_results = []\n",
    "    \n",
    "    for doc in advice_docs:\n",
    "        doc_id = doc['doc_id']\n",
    "        advice_text = doc['text']\n",
    "        \n",
    "        # Check if we have ground truth for this advice letter\n",
    "        # Try to match by doc_id or extract from metadata\n",
    "        expected_ecli = None\n",
    "        \n",
    "        # Method 1: Try direct doc_id match\n",
    "        if doc_id in ground_truth:\n",
    "            expected_ecli = ground_truth[doc_id]\n",
    "        else:\n",
    "            # Method 2: Try to extract from raw_metadata\n",
    "            metadata = doc.get('raw_metadata')\n",
    "            if metadata:\n",
    "                if isinstance(metadata, dict):\n",
    "                    # Try 'Octopus zaaknummer' key\n",
    "                    zaaknummer = metadata.get('Octopus zaaknummer') or metadata.get('zaaknummer')\n",
    "                    if zaaknummer and str(zaaknummer) in ground_truth:\n",
    "                        expected_ecli = ground_truth[str(zaaknummer)]\n",
    "                elif isinstance(metadata, str):\n",
    "                    # Try to parse JSON string\n",
    "                    try:\n",
    "                        import json\n",
    "                        meta_dict = json.loads(metadata)\n",
    "                        zaaknummer = meta_dict.get('Octopus zaaknummer') or meta_dict.get('zaaknummer')\n",
    "                        if zaaknummer and str(zaaknummer) in ground_truth:\n",
    "                            expected_ecli = ground_truth[str(zaaknummer)]\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        if not expected_ecli:\n",
    "            continue  # Skip if no ground truth\n",
    "        \n",
    "        queries_with_ground_truth += 1\n",
    "        total_queries += 1\n",
    "        \n",
    "        # Get RAG results\n",
    "        predicted_ecli = find_relevant_ecli(advice_text, top_n=top_k, min_score=min_score)\n",
    "        predicted_ecli_numbers = [r['ecli_number'] for r in predicted_ecli]\n",
    "        \n",
    "        if predicted_ecli_numbers:\n",
    "            queries_with_results += 1\n",
    "        \n",
    "        # Calculate metrics\n",
    "        expected_set = set(expected_ecli)\n",
    "        predicted_set = set(predicted_ecli_numbers)\n",
    "        \n",
    "        # Precision: relevant retrieved / total retrieved\n",
    "        if len(predicted_set) > 0:\n",
    "            precision = len(expected_set & predicted_set) / len(predicted_set)\n",
    "        else:\n",
    "            precision = 0.0\n",
    "        \n",
    "        # Recall: relevant retrieved / total relevant\n",
    "        if len(expected_set) > 0:\n",
    "            recall = len(expected_set & predicted_set) / len(expected_set)\n",
    "        else:\n",
    "            recall = 0.0\n",
    "        \n",
    "        # F1 score\n",
    "        if precision + recall > 0:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            f1 = 0.0\n",
    "        \n",
    "        # MRR: Find rank of first relevant result\n",
    "        reciprocal_rank = 0.0\n",
    "        for rank, ecli in enumerate(predicted_ecli_numbers, 1):\n",
    "            if ecli in expected_set:\n",
    "                reciprocal_rank = 1.0 / rank\n",
    "                break\n",
    "        \n",
    "        # Hit at K: At least one relevant result in top K\n",
    "        hit_at_k = 1.0 if (expected_set & predicted_set) else 0.0\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "        reciprocal_ranks.append(reciprocal_rank)\n",
    "        hits_at_k += hit_at_k\n",
    "        \n",
    "        detailed_results.append({\n",
    "            'doc_id': doc_id,\n",
    "            'expected_ecli': expected_ecli,\n",
    "            'predicted_ecli': predicted_ecli_numbers,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'reciprocal_rank': reciprocal_rank,\n",
    "            'hit_at_k': hit_at_k\n",
    "        })\n",
    "    \n",
    "    # Calculate averages\n",
    "    if total_queries > 0:\n",
    "        avg_precision = total_precision / total_queries\n",
    "        avg_recall = total_recall / total_queries\n",
    "        avg_f1 = total_f1 / total_queries\n",
    "        mrr = sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "        hit_rate_at_k = hits_at_k / total_queries\n",
    "    else:\n",
    "        avg_precision = avg_recall = avg_f1 = mrr = hit_rate_at_k = 0.0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Evaluation Results\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Total queries evaluated: {total_queries}\")\n",
    "    print(f\"  Queries with ground truth: {queries_with_ground_truth}\")\n",
    "    print(f\"  Queries with results: {queries_with_results} ({queries_with_results/total_queries*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nAccuracy Metrics:\")\n",
    "    print(f\"  Precision@{top_k}: {avg_precision:.4f} ({avg_precision*100:.2f}%)\")\n",
    "    print(f\"  Recall@{top_k}: {avg_recall:.4f} ({avg_recall*100:.2f}%)\")\n",
    "    print(f\"  F1 Score@{top_k}: {avg_f1:.4f}\")\n",
    "    print(f\"  MRR (Mean Reciprocal Rank): {mrr:.4f}\")\n",
    "    print(f\"  Hit Rate@{top_k}: {hit_rate_at_k:.4f} ({hit_rate_at_k*100:.2f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'total_queries': total_queries,\n",
    "        'queries_with_results': queries_with_results,\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1': avg_f1,\n",
    "        'mrr': mrr,\n",
    "        'hit_rate_at_k': hit_rate_at_k,\n",
    "        'detailed_results': detailed_results\n",
    "    }\n",
    "\n",
    "def evaluate_rag_system(test_queries=None, top_k=10):\n",
    "    \"\"\"\n",
    "    Evaluate the RAG system on test queries.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    test_queries : list, optional\n",
    "        List of test queries (advice letter texts). If None, uses sample queries.\n",
    "    top_k : int\n",
    "        Number of ECLI results to retrieve per query\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Evaluation metrics including precision, recall, and MRR\n",
    "    \"\"\"\n",
    "    if test_queries is None:\n",
    "        # Sample test queries from advice letters\n",
    "        with engine.connect() as conn:\n",
    "            sample_advice = conn.execute(sqltext(\"\"\"\n",
    "                SELECT text FROM documents \n",
    "                WHERE doc_type = 'advice' \n",
    "                LIMIT 5\n",
    "            \"\"\")).fetchall()\n",
    "            test_queries = [row[0][:500] for row in sample_advice if row[0]]  # First 500 chars\n",
    "    \n",
    "    print(f\"Evaluating RAG system on {len(test_queries)} queries...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = []\n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\nQuery {i}/{len(test_queries)}: {query[:100]}...\")\n",
    "        ecli_results = find_relevant_ecli(query, top_n=top_k, min_score=0.2)\n",
    "        results.append({\n",
    "            'query': query[:200],\n",
    "            'num_results': len(ecli_results),\n",
    "            'results': ecli_results\n",
    "        })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_queries = len(results)\n",
    "    queries_with_results = sum(1 for r in results if r['num_results'] > 0)\n",
    "    avg_results_per_query = sum(r['num_results'] for r in results) / total_queries if total_queries > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total queries: {total_queries}\")\n",
    "    print(f\"Queries with results: {queries_with_results} ({queries_with_results/total_queries*100:.1f}%)\")\n",
    "    print(f\"Average results per query: {avg_results_per_query:.2f}\")\n",
    "    \n",
    "    # Show score distribution\n",
    "    all_scores = []\n",
    "    for r in results:\n",
    "        all_scores.extend([res['score'] for res in r['results']])\n",
    "    \n",
    "    if all_scores:\n",
    "        print(f\"\\nScore Statistics:\")\n",
    "        print(f\"  Min score: {min(all_scores):.4f}\")\n",
    "        print(f\"  Max score: {max(all_scores):.4f}\")\n",
    "        print(f\"  Mean score: {sum(all_scores)/len(all_scores):.4f}\")\n",
    "        print(f\"  Median score: {sorted(all_scores)[len(all_scores)//2]:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'total_queries': total_queries,\n",
    "        'queries_with_results': queries_with_results,\n",
    "        'avg_results_per_query': avg_results_per_query,\n",
    "        'score_stats': {\n",
    "            'min': min(all_scores) if all_scores else 0,\n",
    "            'max': max(all_scores) if all_scores else 0,\n",
    "            'mean': sum(all_scores)/len(all_scores) if all_scores else 0,\n",
    "        },\n",
    "        'detailed_results': results\n",
    "    }\n",
    "\n",
    "def evaluate_single_query(advice_text, expected_ecli=None, top_k=10):\n",
    "    \"\"\"\n",
    "    Evaluate a single query and optionally check if expected ECLI is in results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    advice_text : str\n",
    "        Advice letter text\n",
    "    expected_ecli : str, optional\n",
    "        Expected ECLI number to find\n",
    "    top_k : int\n",
    "        Number of results to retrieve\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Evaluation results\n",
    "    \"\"\"\n",
    "    results = find_relevant_ecli(advice_text, top_n=top_k, min_score=0.2)\n",
    "    \n",
    "    print(f\"Query: {advice_text[:200]}...\")\n",
    "    print(f\"\\nFound {len(results)} relevant ECLI numbers:\")\n",
    "    \n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"  {i}. {r['ecli_number']} (score: {r['score']:.4f})\")\n",
    "    \n",
    "    eval_result = {\n",
    "        'num_results': len(results),\n",
    "        'results': results,\n",
    "        'expected_found': False\n",
    "    }\n",
    "    \n",
    "    if expected_ecli:\n",
    "        found_ecli = [r['ecli_number'] for r in results]\n",
    "        eval_result['expected_found'] = expected_ecli in found_ecli\n",
    "        if eval_result['expected_found']:\n",
    "            print(f\"\\n‚úì Expected ECLI {expected_ecli} found in results!\")\n",
    "        else:\n",
    "            print(f\"\\n‚úó Expected ECLI {expected_ecli} not found in results\")\n",
    "    \n",
    "    return eval_result\n",
    "\n",
    "print(\"‚úì Evaluation functions ready!\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"How to Test and Calculate Accuracy:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. üéØ Test ALL Advice Letters (Recommended for Full Accuracy):\")\n",
    "print(\"   metrics = evaluate_all_advice_letters(top_k=10, min_score=0.2)\")\n",
    "print(\"   print(f'Overall Accuracy: {metrics[\\\"accuracy\\\"]*100:.2f}%')\")\n",
    "print(\"   # This tests EVERY advice letter with ground truth\")\n",
    "print(\"   # Returns: Hit Rate@K (Accuracy), Precision, Recall, F1, MRR\")\n",
    "print(\"\\n2. Quick Test (Sample of 50):\")\n",
    "print(\"   metrics = evaluate_with_ground_truth(test_size=50, top_k=10)\")\n",
    "print(\"   print(f'Accuracy: {metrics[\\\"hit_rate_at_k\\\"]*100:.2f}%')\")\n",
    "print(\"\\n3. Single Query Test:\")\n",
    "print(\"   result = evaluate_single_query('advice text', expected_ecli='ECLI:NL:...')\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä Key Metrics Explained:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n  üéØ Hit Rate@K (Accuracy):\")\n",
    "print(\"     ‚Üí Percentage of queries with at least one correct ECLI in top K\")\n",
    "print(\"     ‚Üí This is the 'accuracy' you care about\")\n",
    "print(\"\\n  üìä Precision@K:\")\n",
    "print(\"     ‚Üí Of retrieved ECLI, how many are correct\")\n",
    "print(\"\\n  üìà Recall@K:\")\n",
    "print(\"     ‚Üí Of expected ECLI, how many were found\")\n",
    "print(\"\\n  ‚öñÔ∏è  F1 Score:\")\n",
    "print(\"     ‚Üí Harmonic mean of Precision and Recall\")\n",
    "print(\"\\n  üìç MRR:\")\n",
    "print(\"     ‚Üí Average position of first correct result (higher is better)\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Example - Test All Advice Letters:\")\n",
    "print(\"=\"*70)\n",
    "print(\"  # Test all advice letters and get overall accuracy\")\n",
    "print(\"  metrics = evaluate_all_advice_letters(top_k=10, min_score=0.2)\")\n",
    "print(\"  \")\n",
    "print(\"  print(f'\\\\nüéØ Overall Accuracy: {metrics[\\\"accuracy\\\"]*100:.2f}%')\")\n",
    "print(\"  print(f'üìä Precision: {metrics[\\\"precision\\\"]*100:.2f}%')\")\n",
    "print(\"  print(f'üìà Recall: {metrics[\\\"recall\\\"]*100:.2f}%')\")\n",
    "print(\"  print(f'‚öñÔ∏è  F1 Score: {metrics[\\\"f1\\\"]:.4f}')\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "000cea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RAG System Test and Evaluation\n",
      "======================================================================\n",
      "\n",
      "1. Checking PostgreSQL connection and data status...\n",
      "   ‚úì PostgreSQL connection successful\n",
      "   ‚úì Database tables exist: ['chunks', 'documents']\n",
      "   ‚úì ECLI documents: 2447\n",
      "   ‚úì Advice documents: 567\n",
      "   ‚úì Total chunks: 27321\n",
      "\n",
      "2. Testing single query...\n",
      "   Query: Advies van de bezwaarschriftencommissie Juridisch Bureau\n",
      "\n",
      "Aan          Het college van burgemeester ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded ground truth for 567 advice letters\n",
      "   ‚úì Found 5 relevant ECLI numbers\n",
      "   Top result: ECLI:NL:RVS:2018:2845 (score: 1.0000)\n",
      "\n",
      "3. Running full evaluation with ground truth...\n",
      "   This may take a few minutes depending on data size...\n",
      "   ------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Evaluating RAG System with Ground Truth\n",
      "======================================================================\n",
      "Test size: 50\n",
      "Top-K: 10\n",
      "Minimum score: 0.2\n",
      "Ground truth coverage: 567/50 advice letters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Reranker Debug: Reranking 14 candidates\n",
      "   Original top 3 scores: ['1.0000', '0.9397', '0.8957']\n",
      "   Reranked top 3 scores: ['0.7894', '0.7746', '0.7419']\n",
      "\n",
      "======================================================================\n",
      "Evaluation Results\n",
      "======================================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total queries evaluated: 50\n",
      "  Queries with ground truth: 50\n",
      "  Queries with results: 50 (100.0%)\n",
      "\n",
      "Accuracy Metrics:\n",
      "  Precision@10: 0.0600 (6.00%)\n",
      "  Recall@10: 0.3025 (30.25%)\n",
      "  F1 Score@10: 0.0921\n",
      "  MRR (Mean Reciprocal Rank): 0.1773\n",
      "  Hit Rate@10: 0.3400 (34.00%)\n",
      "\n",
      "======================================================================\n",
      "FINAL EVALUATION RESULTS\n",
      "======================================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total queries evaluated: 50\n",
      "  Queries with results: 50 (100.0%)\n",
      "\n",
      "Accuracy Metrics:\n",
      "  Hit Rate@10 (Accuracy): 34.00%\n",
      "    ‚Üí Percentage of queries with at least one correct ECLI in top 10\n",
      "  Precision@10: 6.00%\n",
      "    ‚Üí Of retrieved ECLI, 6.00% are correct\n",
      "  Recall@10: 30.25%\n",
      "    ‚Üí Of correct ECLI, 30.25% were retrieved\n",
      "  F1 Score: 0.0921\n",
      "    ‚Üí Harmonic mean of Precision and Recall\n",
      "  MRR (Mean Reciprocal Rank): 0.1773\n",
      "    ‚Üí Average position of first correct result (higher is better)\n",
      "\n",
      "======================================================================\n",
      "Interpretation:\n",
      "======================================================================\n",
      "  ‚ö† Low accuracy. Consider:\n",
      "    - Lowering min_score threshold\n",
      "    - Checking if ECLI data matches advice letter topics\n",
      "    - Verifying ground truth data quality\n",
      "  ‚ö† Low ranking. Correct results appear late in the list\n",
      "\n",
      "======================================================================\n",
      "Test Complete!\n",
      "======================================================================\n",
      "\n",
      "To access detailed results:\n",
      "  metrics['detailed_results']  # List of all query results\n",
      "  metrics['hit_rate_at_k']     # Accuracy percentage\n",
      "  metrics['precision']         # Precision score\n",
      "  metrics['recall']             # Recall score\n",
      "\n",
      "‚úì All tests completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Complete System Test and Evaluation\n",
    "# ============================================\n",
    "# Run this cell to test the entire RAG system and get accuracy metrics\n",
    "# Make sure you have:\n",
    "# 1. Started PostgreSQL (see Cell 2 for instructions)\n",
    "# 2. Imported data using import_all_data() (Cell 7)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RAG System Test and Evaluation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Step 1: Check PostgreSQL connection and data status\n",
    "print(\"\\n1. Checking PostgreSQL connection and data status...\")\n",
    "\n",
    "# First check if engine exists\n",
    "if engine is None:\n",
    "    print(\"   ‚úó Database engine is None\")\n",
    "    print(\"\\n   \" + \"=\"*66)\n",
    "    print(\"   PostgreSQL Setup Required\")\n",
    "    print(\"   \" + \"=\"*66)\n",
    "    print(\"\\n   Please run Cell 2 first to setup the database.\")\n",
    "    print(\"   If PostgreSQL is not installed, choose one option:\\n\")\n",
    "    print(\"   Option 1: Use Docker (Recommended - Easiest)\")\n",
    "    print(\"   \" + \"-\"*66)\n",
    "    print(\"   Run in terminal:\")\n",
    "    print(\"   docker run -d --name postgres-ecli -p 5432:5432 \\\\\")\n",
    "    print(\"     -e POSTGRES_PASSWORD=postgres postgres:15\")\n",
    "    print(\"   docker exec -it postgres-ecli psql -U postgres -c 'CREATE DATABASE ecli;'\")\n",
    "    print(\"   docker exec -it postgres-ecli psql -U postgres -d ecli -c 'CREATE EXTENSION vector;'\")\n",
    "    print(\"\\n   Option 2: System Installation\")\n",
    "    print(\"   \" + \"-\"*66)\n",
    "    print(\"   sudo apt-get install postgresql postgresql-contrib\")\n",
    "    print(\"   sudo systemctl start postgresql\")\n",
    "    print(\"   sudo -u postgres createdb ecli\")\n",
    "    print(\"   sudo -u postgres psql -d ecli -c 'CREATE EXTENSION vector;'\")\n",
    "    print(\"\\n   After starting PostgreSQL, re-run Cell 2, then Cell 7 (import_all_data())\")\n",
    "    print(\"   \" + \"=\"*66)\n",
    "    raise Exception(\"PostgreSQL not configured. Please run Cell 2 first.\")\n",
    "\n",
    "# Try to connect and check tables\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        # First check if tables exist\n",
    "        tables_check = conn.execute(sqltext(\"\"\"\n",
    "            SELECT table_name \n",
    "            FROM information_schema.tables \n",
    "            WHERE table_schema = 'public' \n",
    "            AND table_name IN ('documents', 'chunks')\n",
    "        \"\"\")).fetchall()\n",
    "        \n",
    "        existing_tables = [row[0] for row in tables_check]\n",
    "        \n",
    "        if 'documents' not in existing_tables or 'chunks' not in existing_tables:\n",
    "            print(f\"   ‚úó Database tables not found\")\n",
    "            print(f\"   Found tables: {existing_tables}\")\n",
    "            print(\"\\n   \" + \"=\"*66)\n",
    "            print(\"   Database Schema Not Created\")\n",
    "            print(\"   \" + \"=\"*66)\n",
    "            print(\"\\n   Please run Cell 2 (Database Setup) first to create tables.\")\n",
    "            print(\"   Cell 2 will create:\")\n",
    "            print(\"   - documents table\")\n",
    "            print(\"   - chunks table\")\n",
    "            print(\"   - indexes and schema\")\n",
    "            print(\"\\n   After running Cell 2, then:\")\n",
    "            print(\"   1. Run Cell 7: import_all_data() to import data\")\n",
    "            print(\"   2. Run this test cell again\")\n",
    "            print(\"   \" + \"=\"*66)\n",
    "            raise Exception(\"Database tables not created. Please run Cell 2 first.\")\n",
    "        \n",
    "        print(f\"   ‚úì PostgreSQL connection successful\")\n",
    "        print(f\"   ‚úì Database tables exist: {existing_tables}\")\n",
    "        \n",
    "        # Now check data\n",
    "        ecli_count = conn.execute(sqltext(\"SELECT COUNT(*) FROM documents WHERE doc_type = 'ecli'\")).scalar()\n",
    "        advice_count = conn.execute(sqltext(\"SELECT COUNT(*) FROM documents WHERE doc_type = 'advice'\")).scalar()\n",
    "        chunk_count = conn.execute(sqltext(\"SELECT COUNT(*) FROM chunks\")).scalar()\n",
    "    \n",
    "    print(f\"   ‚úì ECLI documents: {ecli_count}\")\n",
    "    print(f\"   ‚úì Advice documents: {advice_count}\")\n",
    "    print(f\"   ‚úì Total chunks: {chunk_count}\")\n",
    "    \n",
    "    if ecli_count == 0 or advice_count == 0:\n",
    "        print(\"\\n   ‚ö† No data found! Please run import_all_data() first (Cell 7)\")\n",
    "        print(\"   Example: import_all_data()\")\n",
    "        print(\"\\n   This will import:\")\n",
    "        print(\"   - ECLI data from: DATA ecli_nummers juni 2025 v1 (version 1).xlsx\")\n",
    "        print(\"   - Advice letters from: Dataset Advice letters on objections towing of bicycles.xlsx\")\n",
    "        raise Exception(\"Data not imported. Please run Cell 7: import_all_data()\")\n",
    "    \n",
    "except OperationalError as e:\n",
    "    print(f\"   ‚úó Cannot connect to PostgreSQL\")\n",
    "    print(f\"   Error: {str(e)[:100]}...\")\n",
    "    print(\"\\n   \" + \"=\"*66)\n",
    "    print(\"   PostgreSQL is Not Running\")\n",
    "    print(\"   \" + \"=\"*66)\n",
    "    print(\"\\n   Quick Fix - Use Docker (Recommended):\")\n",
    "    print(\"   \" + \"-\"*66)\n",
    "    print(\"   # Check if container exists\")\n",
    "    print(\"   docker ps -a | grep postgres-ecli\")\n",
    "    print(\"\\n   # If container exists but stopped, start it:\")\n",
    "    print(\"   docker start postgres-ecli\")\n",
    "    print(\"\\n   # If container doesn't exist, create it:\")\n",
    "    print(\"   docker run -d --name postgres-ecli -p 5432:5432 \\\\\")\n",
    "    print(\"     -e POSTGRES_PASSWORD=postgres postgres:15\")\n",
    "    print(\"   docker exec -it postgres-ecli psql -U postgres -c 'CREATE DATABASE ecli;'\")\n",
    "    print(\"   docker exec -it postgres-ecli psql -U postgres -d ecli -c 'CREATE EXTENSION vector;'\")\n",
    "    print(\"\\n   # Or use the setup script:\")\n",
    "    print(\"   bash setup_postgresql.sh\")\n",
    "    print(\"\\n   After starting PostgreSQL:\")\n",
    "    print(\"   1. Re-run Cell 2 (Database Setup)\")\n",
    "    print(\"   2. Run Cell 7: import_all_data()\")\n",
    "    print(\"   3. Then run this test cell again\")\n",
    "    print(\"   \" + \"=\"*66)\n",
    "    raise Exception(\"PostgreSQL connection failed. Please start PostgreSQL first.\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚úó Error: {e}\")\n",
    "    print(\"\\n   Troubleshooting:\")\n",
    "    print(\"   1. Ensure PostgreSQL is running\")\n",
    "    print(\"   2. Run Cell 2 to setup database\")\n",
    "    print(\"   3. Run Cell 7: import_all_data() to import data\")\n",
    "    raise\n",
    "\n",
    "# Step 2: Test single query (optional)\n",
    "print(\"\\n2. Testing single query...\")\n",
    "try:\n",
    "    # Get a sample advice letter for testing\n",
    "    with engine.connect() as conn:\n",
    "        sample_advice = conn.execute(sqltext(\"\"\"\n",
    "            SELECT text FROM documents \n",
    "            WHERE doc_type = 'advice' \n",
    "            LIMIT 1\n",
    "        \"\"\")).scalar()\n",
    "    \n",
    "    if sample_advice:\n",
    "        test_query = sample_advice[:500]  # First 500 characters\n",
    "        print(f\"   Query: {test_query[:100]}...\")\n",
    "        results = find_relevant_ecli(test_query, top_n=5, min_score=0.2)\n",
    "        print(f\"   ‚úì Found {len(results)} relevant ECLI numbers\")\n",
    "        if results:\n",
    "            print(f\"   Top result: {results[0]['ecli_number']} (score: {results[0]['score']:.4f})\")\n",
    "    else:\n",
    "        print(\"   ‚ö† No advice letters found for testing\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö† Single query test failed: {e}\")\n",
    "\n",
    "# Step 3: Full evaluation with ground truth\n",
    "print(\"\\n3. Running full evaluation with ground truth...\")\n",
    "print(\"   This may take a few minutes depending on data size...\")\n",
    "print(\"   \" + \"-\"*66)\n",
    "\n",
    "try:\n",
    "    # Run evaluation\n",
    "    metrics = evaluate_with_ground_truth(test_size=50, top_k=10, min_score=0.2)\n",
    "    \n",
    "    if metrics is None:\n",
    "        print(\"\\n   ‚úó Evaluation failed - check error messages above\")\n",
    "        raise Exception(\"Evaluation returned None\")\n",
    "    \n",
    "    # Print comprehensive results\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL EVALUATION RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Total queries evaluated: {metrics['total_queries']}\")\n",
    "    print(f\"  Queries with results: {metrics['queries_with_results']} ({metrics['queries_with_results']/metrics['total_queries']*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nAccuracy Metrics:\")\n",
    "    print(f\"  Hit Rate@10 (Accuracy): {metrics['hit_rate_at_k']*100:.2f}%\")\n",
    "    print(f\"    ‚Üí Percentage of queries with at least one correct ECLI in top 10\")\n",
    "    print(f\"  Precision@10: {metrics['precision']*100:.2f}%\")\n",
    "    print(f\"    ‚Üí Of retrieved ECLI, {metrics['precision']*100:.2f}% are correct\")\n",
    "    print(f\"  Recall@10: {metrics['recall']*100:.2f}%\")\n",
    "    print(f\"    ‚Üí Of correct ECLI, {metrics['recall']*100:.2f}% were retrieved\")\n",
    "    print(f\"  F1 Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"    ‚Üí Harmonic mean of Precision and Recall\")\n",
    "    print(f\"  MRR (Mean Reciprocal Rank): {metrics['mrr']:.4f}\")\n",
    "    print(f\"    ‚Üí Average position of first correct result (higher is better)\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Interpretation:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if metrics['hit_rate_at_k'] >= 0.8:\n",
    "        print(\"  ‚úì Excellent! System finds correct ECLI in 80%+ of queries\")\n",
    "    elif metrics['hit_rate_at_k'] >= 0.6:\n",
    "        print(\"  ‚úì Good! System finds correct ECLI in 60%+ of queries\")\n",
    "    elif metrics['hit_rate_at_k'] >= 0.4:\n",
    "        print(\"  ‚ö† Moderate. System finds correct ECLI in 40%+ of queries\")\n",
    "    else:\n",
    "        print(\"  ‚ö† Low accuracy. Consider:\")\n",
    "        print(\"    - Lowering min_score threshold\")\n",
    "        print(\"    - Checking if ECLI data matches advice letter topics\")\n",
    "        print(\"    - Verifying ground truth data quality\")\n",
    "    \n",
    "    if metrics['mrr'] >= 0.5:\n",
    "        print(\"  ‚úì Good ranking! Correct results appear early in the list\")\n",
    "    elif metrics['mrr'] >= 0.3:\n",
    "        print(\"  ‚ö† Moderate ranking. Correct results appear in middle positions\")\n",
    "    else:\n",
    "        print(\"  ‚ö† Low ranking. Correct results appear late in the list\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Test Complete!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Return metrics for further analysis\n",
    "    print(\"\\nTo access detailed results:\")\n",
    "    print(\"  metrics['detailed_results']  # List of all query results\")\n",
    "    print(\"  metrics['hit_rate_at_k']     # Accuracy percentage\")\n",
    "    print(\"  metrics['precision']         # Precision score\")\n",
    "    print(\"  metrics['recall']             # Recall score\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó Evaluation failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Ensure PostgreSQL is running\")\n",
    "    print(\"  2. Ensure data is imported: import_all_data()\")\n",
    "    print(\"  3. Check that advice letters Excel file has 'ECLI' column\")\n",
    "    print(\"  4. Verify ground truth loading: load_ground_truth_ecli()\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n‚úì All tests completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb7d2c6",
   "metadata": {},
   "source": [
    "# üöÄ RAG System Quick Start Guide\n",
    "\n",
    "## Overview\n",
    "This notebook implements a complete RAG (Retrieval Augmented Generation) system for finding relevant ECLI numbers for advice letters.\n",
    "\n",
    "## Setup Steps\n",
    "\n",
    "### 1. Install Packages (Cell 0)\n",
    "Run Cell 0 to install all required packages. **Restart kernel after installation.**\n",
    "\n",
    "### 2. Setup Database (Cell 1)\n",
    "Run Cell 1 to create PostgreSQL database schema. Ensure PostgreSQL is running.\n",
    "\n",
    "### 3. Check Data Status (Cell 2)\n",
    "Run Cell 2 to verify database connection and check if data exists.\n",
    "\n",
    "### 4. Import Data (Cell 5)\n",
    "Run `import_all_data()` to import:\n",
    "- ECLI documents (knowledge base)\n",
    "- Advice letters (for testing)\n",
    "\n",
    "### 5. Test System (Cell 8)\n",
    "Use `test_with_existing_advice_letter()` to test with existing data.\n",
    "\n",
    "### 6. Evaluate Accuracy (Cell 7)\n",
    "Use `evaluate_with_ground_truth()` to calculate accuracy metrics.\n",
    "\n",
    "## Key Functions\n",
    "\n",
    "- `find_relevant_ecli(advice_text, top_n=10)`: Find relevant ECLI numbers\n",
    "- `test_with_existing_advice_letter(advice_index=None)`: Test with existing advice letter\n",
    "- `evaluate_with_ground_truth(test_size=50, top_k=10)`: Evaluate system accuracy\n",
    "- `read_word_document(docx_path)`: Read Word document for testing\n",
    "\n",
    "## File Requirements\n",
    "\n",
    "- `DATA ecli_nummers juni 2025 v1 (version 1).xlsx`: ECLI knowledge base\n",
    "- `Dataset Advice letters on objections towing of bicycles.xlsx`: Advice letters with ground truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "376798b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_with_existing_advice_letter(advice_index=10, top_n=5, min_score=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b53ea0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Evaluating RAG System on ALL Advice Letters\n",
      "======================================================================\n",
      "Total advice letters in database: 567\n",
      "Ground truth available for: 567 advice letters\n",
      "Top-K: 5\n",
      "Minimum score: 0.3\n",
      "‚úì Reranker: ENABLED (will rerank top 50 candidates)\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating advice letters: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 567/567 [02:45<00:00,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Evaluation Results - ALL Advice Letters\n",
      "======================================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total advice letters in database: 567\n",
      "  Advice letters with ground truth: 567\n",
      "  Advice letters evaluated: 567\n",
      "  Queries with results: 567 (100.0%)\n",
      "\n",
      "======================================================================\n",
      "Accuracy Metrics (Top-5):\n",
      "======================================================================\n",
      "  üéØ Hit Rate@5 (Accuracy): 0.8571 (85.71%)\n",
      "     ‚Üí Percentage of queries with at least one correct ECLI in top 5\n",
      "\n",
      "  üìä Precision@5: 0.1912 (19.12%)\n",
      "     ‚Üí Of retrieved ECLI, how many are correct\n",
      "\n",
      "  üìà Recall@5: 0.8300 (83.00%)\n",
      "     ‚Üí Of expected ECLI, how many were found\n",
      "\n",
      "  ‚öñÔ∏è  F1 Score@5: 0.3058\n",
      "     ‚Üí Harmonic mean of Precision and Recall\n",
      "\n",
      "  üìç MRR (Mean Reciprocal Rank): 0.4346\n",
      "     ‚Üí Average position of first correct result (higher is better)\n",
      "\n",
      "======================================================================\n",
      "Additional Statistics:\n",
      "======================================================================\n",
      "  Queries with single ECLI in ground truth: 481\n",
      "  Queries with multiple ECLI in ground truth: 86\n",
      "  Average expected ECLI per query: 1.18\n",
      "  Average predicted ECLI per query: 5.00\n",
      "\n",
      "======================================================================\n",
      "Sample Results (First 5 queries):\n",
      "======================================================================\n",
      "\n",
      "Query 1: JB.22.017983.001\n",
      "  Expected: 1 ECLI\n",
      "  Predicted: 5 ECLI\n",
      "  Hit@K: ‚úó\n",
      "  Precision: 0.00, Recall: 0.00\n",
      "\n",
      "Query 2: JB.20.017560.001\n",
      "  Expected: 1 ECLI\n",
      "  Predicted: 5 ECLI\n",
      "  Hit@K: ‚úó\n",
      "  Precision: 0.00, Recall: 0.00\n",
      "\n",
      "Query 3: JB.21.014183.001\n",
      "  Expected: 1 ECLI\n",
      "  Predicted: 5 ECLI\n",
      "  Hit@K: ‚úó\n",
      "  Precision: 0.00, Recall: 0.00\n",
      "\n",
      "Query 4: JB.22.011582.001\n",
      "  Expected: 1 ECLI\n",
      "  Predicted: 5 ECLI\n",
      "  Hit@K: ‚úó\n",
      "  Precision: 0.00, Recall: 0.00\n",
      "\n",
      "Query 5: JB.19.018218.001\n",
      "  Expected: 1 ECLI\n",
      "  Predicted: 5 ECLI\n",
      "  Hit@K: ‚úó\n",
      "  Precision: 0.00, Recall: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Test Reranker Functionality\n",
    "# ============================================\n",
    "# Get advice_text from database or use test function\n",
    "\n",
    "# Method 1: Use test function (recommended - automatically gets advice_text)\n",
    "#test_with_existing_advice_letter(advice_index=0, top_n=5, min_score=0.3)\n",
    "\n",
    "# Method 2: Get advice_text from database manually\n",
    "# from sqlalchemy import text\n",
    "# with engine.connect() as conn:\n",
    "#     result = conn.execute(text(\"\"\"\n",
    "#         SELECT text FROM documents \n",
    "#         WHERE doc_type = 'advice' \n",
    "#         LIMIT 1\n",
    "#     \"\"\")).fetchone()\n",
    "#     if result:\n",
    "#         advice_text = result[0]\n",
    "#         # Test with reranker\n",
    "#         results = find_relevant_ecli(advice_text, top_n=5, rerank_top_k=50, use_reranker=True)\n",
    "#         print(format_ecli_citations(results))\n",
    "#     else:\n",
    "#         print(\"No advice letters found in database\")\n",
    "\n",
    "# Method 3: Test with sample text\n",
    "# advice_text = \"bezwaar tegen wegnemen fiets parkeerverbod\"\n",
    "# results = find_relevant_ecli(advice_text, top_n=5, rerank_top_k=50)\n",
    "# print(format_ecli_citations(results))\n",
    "\n",
    "# Method 4: Full evaluation with reranker\n",
    "metrics = evaluate_all_advice_letters(top_k=5, min_score=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71f5f23",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'advice_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m find_relevant_ecli_by_issues(\u001b[43madvice_text\u001b[49m, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(format_ecli_citations(results, show_evidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'advice_text' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Test Issues-Based Retrieval Strategy\n",
    "# ============================================\n",
    "# Get advice_text from database\n",
    "\n",
    "from sqlalchemy import text as sqltext\n",
    "\n",
    "# Get an advice letter from database\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(sqltext(\"\"\"\n",
    "        SELECT text FROM documents \n",
    "        WHERE doc_type = 'advice' \n",
    "        LIMIT 1\n",
    "    \"\"\")).fetchone()\n",
    "    \n",
    "    if result:\n",
    "        advice_text = result[0]\n",
    "        print(\"=\"*70)\n",
    "        print(\"Testing Issues-Based Retrieval Strategy\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nAdvice Letter Preview: {advice_text[:200]}...\")\n",
    "        print(f\"Total length: {len(advice_text)} characters\\n\")\n",
    "        \n",
    "        # Test with issues-based retrieval\n",
    "        results = find_relevant_ecli_by_issues(advice_text, top_n=5)\n",
    "        print(format_ecli_citations(results, show_evidence=True))\n",
    "    else:\n",
    "        print(\"‚ö† No advice letters found in database. Please run import_all_data() first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4a51a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_parameter_combinations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6c0b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze_keyword_filtering_impact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5faefae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear_all_chunks(confirm=True)\n",
    "#clear_all_data(confirm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a99657b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
